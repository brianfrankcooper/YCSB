/**
 * Copyright (c) 2017 YCSB contributors All rights reserved.
 * <p>
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */
package site.ycsb.workloads;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Properties;
import java.util.Random;
import java.util.Set;
import java.util.stream.IntStream;
import java.util.stream.Collectors;
import java.util.TreeMap;
import java.util.Vector;
import java.util.concurrent.ThreadLocalRandom;
import java.util.concurrent.TimeUnit;

import site.ycsb.ByteIterator;
import site.ycsb.Client;
import site.ycsb.DB;
import site.ycsb.NumericByteIterator;
import site.ycsb.Status;
import site.ycsb.StringByteIterator;
import site.ycsb.Utils;
import site.ycsb.Workload;
import site.ycsb.WorkloadException;
import site.ycsb.generator.DiscreteGenerator;
import site.ycsb.generator.Generator;
import site.ycsb.generator.HotspotIntegerGenerator;
import site.ycsb.generator.IncrementingPrintableStringGenerator;
import site.ycsb.generator.NumberGenerator;
import site.ycsb.generator.RandomDiscreteTimestampGenerator;
import site.ycsb.generator.ScrambledZipfianGenerator;
import site.ycsb.generator.SequentialGenerator;
import site.ycsb.generator.UniformLongGenerator;
import site.ycsb.generator.UnixEpochTimestampGenerator;
import site.ycsb.generator.ZipfianGenerator;
import site.ycsb.measurements.Measurements;

/**
 * A specialized workload dealing with time series data, i.e. series of discreet
 * events associated with timestamps and identifiers. For this workload, identities
 * consist of a {@link String} <b>key</b> and a set of {@link String} <b>tag key/value</b>
 * pairs. 
 * <p>
 * For example:
 * <table border="1">
 * <tr><th>Time Series Key</th><th>Tag Keys/Values</th><th>1483228800</th><th>1483228860</th><th>1483228920</th></tr>
 * <tr><td>AA</td><td>AA=AA, AB=AA</td><td>42.5</td><td>1.0</td><td>85.9</td></tr>
 * <tr><td>AA</td><td>AA=AA, AB=AB</td><td>-9.4</td><td>76.9</td><td>0.18</td></tr>
 * <tr><td>AB</td><td>AA=AA, AB=AA</td><td>-93.0</td><td>57.1</td><td>-63.8</td></tr>
 * <tr><td>AB</td><td>AA=AA, AB=AB</td><td>7.6</td><td>56.1</td><td>-0.3</td></tr>
 * </table>
 * <p>
 * This table shows four time series with 3 measurements at three different timestamps.
 * Keys, tags, timestamps and values (numeric only at this time) are generated by
 * this workload. For details on properties and behavior, see  the 
 * {@code workloads/tsworkload_template} file. The Javadocs will focus on implementation
 * and how {@link DB} clients can parse the workload.
 * <p>
 * In order to avoid having existing DB implementations implement a brand new interface
 * this workload uses the existing APIs to encode a few special values that can be parsed
 * by the client. The special values include the timestamp, numeric value and some
 * query (read or scan) parameters. As an example on how to parse the fields, see
 * {@link BasicTSDB}.
 * <p>
 * <b>Timestamps</b>
 * <p>
 * Timestamps are presented as Unix Epoch values in units of {@link TimeUnit#SECONDS},
 * {@link TimeUnit#MILLISECONDS} or {@link TimeUnit#NANOSECONDS} based on the 
 * {@code timestampunits} property. For calls to {@link DB#insert(String, String, java.util.Map)}
 * and {@link DB#update(String, String, java.util.Map)}, the timestamp is added to the 
 * {@code values} map encoded in a {@link NumericByteIterator} with the key defined
 * in the {@code timestampkey} property (defaulting to "YCSBTS"). To pull out the timestamp
 * when iterating over the values map, cast the {@link ByteIterator} to a 
 * {@link NumericByteIterator} and call {@link NumericByteIterator#getLong()}.
 * <p>
 * Note that for calls to {@link DB#update(String, String, java.util.Map)}, timestamps
 * earlier than the timestamp generator's timestamp will be choosen at random to
 * mimic a lambda architecture or old job re-reporting some data.
 * <p>
 * For calls to {@link DB#read(String, String, java.util.Set, java.util.Map)} and 
 * {@link DB#scan(String, String, int, java.util.Set, Vector)}, timestamps
 * are encoded in a {@link StringByteIterator} in a key/value format with the 
 * {@code tagpairdelimiter} separator. E.g {@code YCSBTS=1483228800}. If {@code querytimespan}
 * has been set to a positive value then the value will include a range with the
 * starting (oldest) timestamp followed by the {@code querytimespandelimiter} separator
 * and the ending (most recent) timestamp. E.g. {@code YCSBTS=1483228800-1483228920}.
 * <p>
 * For calls to {@link DB#delete(String, String)}, encoding is the same as reads and 
 * scans but key/value pairs are separated by the {@code deletedelimiter} property value.
 * <p>
 * By default, the starting timestamp is the current system time without any rounding.
 * All timestamps are then offsets from that starting value.
 * <p>
 * <b>Values</b>
 * <p>
 * Similar to timestamps, values are encoded in {@link NumericByteIterator}s and stored
 * in the values map with the key defined in {@code valuekey} (defaulting to "YCSBV").
 * Values can either be 64 bit signed {@link long}s or double precision {@link double}s 
 * depending on the {@code valuetype} or {@code dataintegrity} properties. When parsing
 * out the value, always call {@link NumericByteIterator#isFloatingPoint()} to determine
 * whether or not to call {@link NumericByteIterator#getDouble()} (true) or 
 * {@link NumericByteIterator#getLong()} (false). 
 * <p>
 * When {@code dataintegrity} is set to true, then the value is always set to a 
 * 64 bit signed integer which is the Java hash code of the concatenation of the
 * key and map of values (sorted on the map keys and skipping the timestamp and value
 * entries) OR'd with the timestamp of the data point. See 
 * {@link #validationFunction(String, long, TreeMap)} for the implementation.
 * <p>
 * <b>Keys and Tags</b>
 * <p>
 * As mentioned, the workload generates strings for the keys and tags. On initialization
 * three string generators are created using the {@link IncrementingPrintableStringGenerator} 
 * implementation. Then the generators fill three arrays with values based on the
 * number of keys, the number of tags and the cardinality of each tag key/value pair.
 * This implementation gives us time series like the example table where every string
 * starts at something like "AA" (depending on the length of keys, tag keys and tag values)
 * and continuing to "ZZ" wherein they rollover back to "AA". 
 * <p>
 * Each time series must have a unique set of tag keys, i.e. the key "AA" cannot appear
 * more than once per time series. If the workload is configured for four tags with a
 * tag key length of 2, the keys would be "AA", "AB", "AC" and "AD". 
 * <p>
 * Each tag key is then associated with a tag value. Tag values may appear more than once
 * in each time series. E.g. time series will usually start with the tags "AA=AA", 
 * "AB=AA", "AC=AA" and "AD=AA". The {@code tagcardinality} property determines how many
 * unique values will be generated per tag key. In the example table above, the 
 * {@code tagcardinality} property would have been set to {@code 1,2} meaning tag 
 * key "AA" would always have the tag value "AA" given a cardinality of 1. However 
 * tag key "AB" would have values "AA" and "AB" due to a cardinality of 2. This 
 * cardinality map, along with the number of unique time series keys determines how 
 * many unique time series are generated for the workload. Tag values share a common
 * array of generated strings to save on memory.
 * <p>
 * <b>Operation Order</b>
 * <p>
 * The default behavior of the workload (for inserts and updates) is to generate a
 * value for each time series for a given timestamp before incrementing to the next
 * timestamp and writing values. This is an ideal workload and some time series 
 * databases are designed for this behavior. However in the real-world events will
 * arrive grouped close to the current system time with a number of events being 
 * delayed, hence their timestamps are further in the past. The {@code delayedseries}
 * property determines the percentage of time series that are delayed by up to
 * {@code delayedintervals} intervals. E.g. setting this value to 0.05 means that 
 * 5% of the time series will be written with timestamps earlier than the timestamp
 * generator's current time.
 * </p>
 * <b>Reads and Scans</b>
 * <p>
 * For benchmarking queries, some common tasks implemented by almost every time series
 * data base are available and are passed in the fields {@link Set}:
 * <p>
 * <b>GroupBy</b> - A common operation is to aggregate multiple time series into a
 * single time series via common parameters. For example, a user may want to see the
 * total network traffic in a data center so they'll issue a SQL query like:
 * <code>SELECT value FROM timeseriesdb GROUP BY datacenter ORDER BY SUM(value);</code>
 * If the {@code groupbyfunction} has been set to a group by function, then the fields
 * will contain a key/value pair with the key set in {@code groupbykey}. E.g.
 * {@code YCSBGB=SUM}.
 * <p>
 * Additionally with grouping enabled, fields on tag keys where group bys should 
 * occur will only have the key defined and will not have a value or delimiter. E.g.
 * if grouping on tag key "AA", the field will contain {@code AA} instead of {@code AA=AB}.
 * <p>
 * <b>Downsampling</b> - Another common operation is to reduce the resolution of the
 * queried time series when fetching a wide time range of data so fewer data points 
 * are returned. For example, a user may fetch a week of data but if the data is
 * recorded on a 1 second interval, that would be over 600k data points so they
 * may ask for a 1 hour downsampling (also called bucketing) wherein every hour, all
 * of the data points for a "bucket" are aggregated into a single value. 
 * <p>
 * To enable downsampling, the {@code downsamplingfunction} property must be set to
 * a supported function such as "SUM" and the {@code downsamplinginterval} must be
 * set to a valid time interval with the same units as {@code timestampunits}, e.g.
 * "3600" which would create 1 hour buckets if the time units were set to seconds.
 * With downsampling, query fields will include a key/value pair with 
 * {@code downsamplingkey} as the key (defaulting to "YCSBDS") and the value being
 * a concatenation of {@code downsamplingfunction} and {@code downsamplinginterval},
 * for example {@code YCSBDS=SUM60}.
 * <p>
 * <b>Timestamps</b> - For every read, a random timestamp is selected from the interval
 * set. If {@code querytimespan} has been set to a positive value, then the configured
 * query time interval is added to the selected timestamp so the read passes the DB
 * a range of times. Note that during the run phase, if no data was previously loaded,
 * or if there are more {@code recordcount}s set for the run phase, reads may be sent
 * to the DB with timestamps that are beyond the written data time range (or even the
 * system clock of the DB).
 * <p>
 * <b>Deletes</b>
 * <p>
 * Because the delete API only accepts a single key, a full key and tag key/value 
 * pair map is flattened into a single string for parsing by the database. Common
 * workloads include deleting a single time series (wherein all tag key and values are
 * defined), deleting all series containing a tag key and value or deleting all of the
 * time series sharing a common time series key. 
 * <p>
 * Right now the workload supports deletes with a key and for time series tag key/value
 * pairs or a key with tags and a group by on one or more tags (meaning, delete all of
 * the series with any value for the given tag key). The parameters are collapsed into
 * a single string delimited with the character in the {@code deletedelimiter} property.
 * For example, a delete request may look like: {@code AA:AA=AA:AA=AB} to delete the 
 * first time series in the table above.
 * <p>
 * <b>Threads</b>
 * <p>
 * For a multi-threaded execution, the number of time series keys set via the 
 * {@code fieldcount} property, must be greater than or equal to the number of
 * threads set via {@code threads}. This is due to each thread choosing a subset
 * of the total number of time series keys and being responsible for writing values 
 * for each time series containing those keys at each timestamp. Thus each thread
 * will have it's own timestamp generator, incrementing each time every time series
 * it is responsible for has had a value written.
 * <p>
 * Each thread may, however, issue reads and scans for any time series in the 
 * complete set.
 * <p>
 * <b>Sparsity</b>
 * <p>
 * By default, during loads, every time series will have a data point written at every
 * time stamp in the interval set. This is common in workloads where a sensor writes
 * a value at regular intervals. However some time series are only reported under 
 * certain conditions. 
 * <p>
 * For example, a counter may track the number of errors over a 
 * time period for a web service and only report when the value is greater than 1.
 * Or a time series may include tags such as a user ID and IP address when a request
 * arrives at the web service and only report values when that combination is seen.
 * This means the timeseries will <i>not</i> have a value at every timestamp and in
 * some cases there may be only a single value!  
 * <p>
 * This workload has a {@code sparsity} parameter that can choose how often a 
 * time series should record a value. The default value of 0.0 means every series 
 * will get a value at every timestamp. A value of 0.95 will mean that for each 
 * series, only 5% of the timestamps in the interval will have a value. The distribution
 * of values is random.
 * <p>
 * <b>Notes/Warnings</b>
 * <p>
 * <ul>
 * <li>Because time series keys and tag key/values are generated and stored in memory,
 * be careful of setting the cardinality too high for the JVM's heap.</li>
 * <li>When running for data integrity, a number of settings are incompatible and will
 * throw errors. Check the error messages for details.</li>
 * <li>Databases that support keys only and can't store tags should order and then 
 * collapse the tag values using a delimiter. For example the series in the example 
 * table at the top could be written as:
 * <ul>
 * <li>{@code AA.AA.AA}</li>
 * <li>{@code AA.AA.AB}</li>
 * <li>{@code AB.AA.AA}</li>
 * <li>{@code AB.AA.AB}</li>
 * </ul></li>
 * </ul>
 * <p>
 * <b>TODOs</b>
 * <p>
 * <ul>
 * <li>Support random time intervals. E.g. some series write every second, others every
 * 60 seconds.</li>
 * <li>Support random time series cardinality. Right now every series has the same 
 * cardinality.</li>
 * <li>Truly random timetamps per time series. We could use bitmaps to determine if
 * a series has had a value written for a given timestamp. Right now all of the series
 * are in sync time-wise.</li>
 * <li>Possibly a real-time load where values are written with the current system time.
 * It's more of a bulk-loading operation now.</li>
 * </ul>
 */
public class TimeSeriesWorkload extends Workload {  
  
  /**
   * The types of values written to the timeseries store.
   */
  public enum ValueType {
    INTEGERS("integers"),
    FLOATS("floats"),
    MIXED("mixednumbers");
    
    protected final String name;
    
    ValueType(final String name) {
      this.name = name;
    }
    
    public static ValueType fromString(final String name) {
      for (final ValueType type : ValueType.values()) {
        if (type.name.equalsIgnoreCase(name)) {
          return type;
        }
      }
      throw new IllegalArgumentException("Unrecognized type: " + name);
    }
  }
  
  /** Name for the insert transaction start property
   *  Should represent a unixstamp (in the units set by timestampunits property)
   *  that indicates which timestamp should be considered the starting
   *  timestamp for all insert queries that are performed *during a transaction run*.
   *  This is a complement to the existing insertstart property which determines
   *  the starting timestamp for a load run, and also is then the starting
   *  timestamp for read/scan queries in a transaction run.
   *  This is intended to be used when first loading data with a load run,
   *  then running a mixed workload with read and/or scan queries, but
   *  *also* more insert queries. Then the inserttransactionstart property would
   *  indicate where the *new* inserts that occur during the transaction run
   *  should start from.
   * */
  public static final String INSERT_TRANSACTION_START_PROPERTY = "inserttransactionstart";

  /** Name for the read start property
   *  Should represent a unixstamp (in the units set by timestampunits property)
   *  that indicates which timestamp should be considered the starting
   *  timestamp for all read/scan/update queries that are performed *during a transaction run*.
   *  This is a complement to the existing insertstart property which determines
   *  the starting timestamp for a load run, and also is then normally the starting
   *  timestamp for read/scan queries in a transaction run.
   * */
  public static final String READ_START_PROPERTY = "readstart";

  /** Name and default value for the timestamp key property. */
  public static final String TIMESTAMP_KEY_PROPERTY = "timestampkey";
  public static final String TIMESTAMP_KEY_PROPERTY_DEFAULT = "YCSBTS";
  
  /** Name and default value for the value key property. */
  public static final String VALUE_KEY_PROPERTY = "valuekey";
  public static final String VALUE_KEY_PROPERTY_DEFAULT = "YCSBV";
  
  /** Name and default value for the timestamp interval property. */    
  public static final String TIMESTAMP_INTERVAL_PROPERTY = "timestampinterval";    
  public static final String TIMESTAMP_INTERVAL_PROPERTY_DEFAULT = "60";    
      
  /** Name and default value for the timestamp units property. */   
  public static final String TIMESTAMP_UNITS_PROPERTY = "timestampunits";    
  public static final String TIMESTAMP_UNITS_PROPERTY_DEFAULT = "SECONDS"; 
  
  /** Name and default value for the number of tags property. */
  public static final String TAG_COUNT_PROPERTY = "tagcount";
  public static final String TAG_COUNT_PROPERTY_DEFAULT = "4";
  
  /** Name and default value for the tag value cardinality map property. */
  public static final String TAG_CARDINALITY_PROPERTY = "tagcardinality";
  public static final String TAG_CARDINALITY_PROPERTY_DEFAULT = "1, 2, 4, 8";

  /** Name and default value for the threaded write distribution property. */
  public static final String THREADED_WRITE_DISTRIBUTION_PROPERTY = "threadedwritedistribution";
  public static final String THREADED_WRITE_DISTRIBUTION_PROPERTY_DEFAULT = "key";
  
  /** Name and default value for the tag key length property. */
  public static final String TAG_KEY_LENGTH_PROPERTY = "tagkeylength";
  public static final String TAG_KEY_LENGTH_PROPERTY_DEFAULT = "8";
  
  /** Name and default value for the tag value length property. */
  public static final String TAG_VALUE_LENGTH_PROPERTY = "tagvaluelength";
  public static final String TAG_VALUE_LENGTH_PROPERTY_DEFAULT = "8";
  
  /** Name and default value for the tag pair delimiter property. */
  public static final String PAIR_DELIMITER_PROPERTY = "tagpairdelimiter";
  public static final String PAIR_DELIMITER_PROPERTY_DEFAULT = "=";
  
  /** Name and default value for the delete string delimiter property. */
  public static final String DELETE_DELIMITER_PROPERTY = "deletedelimiter";
  public static final String DELETE_DELIMITER_PROPERTY_DEFAULT = ":";
  
  /** Name and default value for the random timestamp write order property. */
  public static final String RANDOMIZE_TIMESTAMP_ORDER_PROPERTY = "randomwritetimestamporder";
  public static final String RANDOMIZE_TIMESTAMP_ORDER_PROPERTY_DEFAULT = "false";
  
  /** Name and default value for the random time series write order property. */
  public static final String RANDOMIZE_TIMESERIES_ORDER_PROPERTY = "randomtimeseriesorder";
  public static final String RANDOMIZE_TIMESERIES_ORDER_PROPERTY_DEFAULT = "true";
  
  /** Name and default value for the value types property. */
  public static final String VALUE_TYPE_PROPERTY = "valuetype";
  public static final String VALUE_TYPE_PROPERTY_DEFAULT = "floats";
  
  /** Name and default value for the sparsity property. */
  public static final String SPARSITY_PROPERTY = "sparsity";
  public static final String SPARSITY_PROPERTY_DEFAULT = "0.00";
  
  /** Name and default value for the delayed series percentage property. */
  public static final String DELAYED_SERIES_PROPERTY = "delayedseries";
  public static final String DELAYED_SERIES_PROPERTY_DEFAULT = "0.10";
  
  /** Name and default value for the delayed series intervals property. */
  public static final String DELAYED_INTERVALS_PROPERTY = "delayedintervals";
  public static final String DELAYED_INTERVALS_PROPERTY_DEFAULT = "5";

  /** Name and default value for the query time stamp delimiter property (used with SCAN and DELETE queries). */
  public static final String QUERY_TIMESPAN_DELIMITER_PROPERTY = "querytimespandelimiter";
  public static final String QUERY_TIMESPAN_DELIMITER_PROPERTY_DEFAULT = ",";

  /** Name and default value for the group by tags list delimiter property. */
  public static final String GROUPBY_TAGS_LIST_DELIMITER_PROPERTY = "groupbytaglistdelimiter";
  public static final String GROUPBY_TAGS_LIST_DELIMITER_PROPERTY_DEFAULT = ";";

  /** Name  for the SCAN tags property, which defines which tags are given fixed values
   *  during SCAN queries.
   *  Property is optional and therefore has no default value
   *  */
  public static final String SCAN_TAGS_PROPERTY = "scantags";

  /** Name  for the SCAN start timestamp property.
   *  Property is optional and therefore has no default value
   *  */
  public static final String SCAN_START_TS_PROPERTY = "scanstartts";
  
  /** Name and default value for the SCAN query time span property. */
  public static final String SCAN_QUERY_TIMESPAN_PROPERTY = "scanquerytimespan";
  public static final String SCAN_QUERY_TIMESPAN_PROPERTY_DEFAULT = "0";

  /** Name and default value for the SCAN randomized query time span property. */
  public static final String SCAN_QUERY_RANDOM_TIMESPAN_PROPERTY = "scanqueryrandomtimespan";
  public static final String SCAN_QUERY_RANDOM_TIMESPAN_PROPERTY_DEFAULT = "false";
  
  /** Name and default value for group-by key property used to send group-by info in the YCSB standard data model. */
  public static final String GROUPBY_FUNCTION_KEY_PROPERTY = "groupbyfunctionkey";
  public static final String GROUPBY_FUNCTION_KEY_PROPERTY_DEFAULT = "YCSBGBF";

  /** Name and default value for group-by key property used to send group-by info in the YCSB standard data model. */
  public static final String GROUPBY_TAGS_KEY_PROPERTY = "groupbytagskey";
  public static final String GROUPBY_TAGS_KEY_PROPERTY_DEFAULT = "YCSBGBT";

  /** Name  for the READ tags property, which defines which tags are given fixed values
   *  during READ queries.
   *  Property is optional and therefore has no default value
   *  */
  public static final String READ_TAGS_PROPERTY = "readtags";

  /** Name and default value for the READ query group-by function property. */
  public static final String READ_GROUPBY_PROPERTY = "readgroupbyfunction";
  
  /** Name and default value for the READ query group-by key map property. */
  public static final String READ_GROUPBY_TAG_KEYS_PROPERTY = "readgroupbytagkeys";
  
  /** Name and default value for the SCAN query group-by function property. */
  public static final String SCAN_GROUPBY_PROPERTY = "scangroupbyfunction";
  
  /** Name and default value for the SCAN query group-by key map property. */
  public static final String SCAN_GROUPBY_TAG_KEYS_PROPERTY = "scangroupbytagkeys";
  
  /** Name and default value for the SCAN query downsampling key property. */
  public static final String SCAN_DOWNSAMPLING_KEY_PROPERTY = "scandownsamplingkey";
  public static final String SCAN_DOWNSAMPLING_KEY_PROPERTY_DEFAULT = "YCSBDS";
  
  /** Name and default value for the SCAN query downsampling function property. */
  public static final String SCAN_DOWNSAMPLING_FUNCTION_PROPERTY = "scandownsamplingfunction";
  
  /** Name and default value for the SCAN query downsampling interval property. */
  public static final String SCAN_DOWNSAMPLING_INTERVAL_PROPERTY = "scandownsamplinginterval";

  /** Name and default value for the delete tag keys property. */
  public static final String DELETE_TAG_KEYS_PROPERTY = "deletetagkeys";

  /** Name and default value for the DELETE query time span property. */
  public static final String DELETE_QUERY_TIMESPAN_PROPERTY = "deletequerytimespan";
  public static final String DELETE_QUERY_TIMESPAN_PROPERTY_DEFAULT = "0";

  /** Name and default value for the DELETE randomized query time span property. */
  public static final String DELETE_QUERY_RANDOM_TIMESPAN_PROPERTY = "deletequeryrandomtimespan";
  public static final String DELETE_QUERY_RANDOM_TIMESPAN_PROPERTY_DEFAULT = "false";

  /** global debug property loading. */
  private static final String DEBUG_PROPERTY = "debug";
  private static final String DEBUG_PROPERTY_DEFAULT = "false";

  /** class-specific debug property loading. */
  private static final String TIMESERIESWORKLOAD_DEBUG_PROPERTY = "timeseriesworkload.debug";
  
  /** The properties to pull settings from. */
  protected Properties properties;

  /** A boolean representing whether or not we are doing a transaction run or not. */
  protected boolean doTransactions;
  
  /** Generators for keys, tag keys and tag values. */
  protected Generator<String> keyGenerator;
  protected Generator<String> tagKeyGenerator;
  protected Generator<String> tagValueGenerator;
  
  /** The timestamp key, defaults to "YCSBTS". */
  protected String timestampKey;
  
  /** The value key, defaults to "YCSBDS". */
  protected String valueKey;
  
  /** The number of time units in between timestamps. */
  protected int timestampInterval;
  
  /** The units of time the timestamp and various intervals represent. */
  protected TimeUnit timeUnits;
  
  /** Whether or not to randomize the timestamp order when writing. */
  protected boolean randomizeTimestampOrder;
  
  /** Whether or not to randomize (shuffle) the time series order. NOT compatible
   * with data integrity. */
  protected boolean randomizeTimeseriesOrder;
  
  /** The type of values to generate when writing data. */
  protected ValueType valueType;
  
  /** Used to calculate an offset for each time series. */
  protected int[] cumulativeCardinality;
  
  /** Used to when generating the key and tag value indices
   *  across the whole tree of values when splitting up
   *  the generation and insertion of data points into
   *  groups of time series per thread.
   */
  protected int[] cumulativeTotalCardinality;
  
  /** The calculated total cardinality based on the config. */
  protected int totalCardinality;
  
  /** The calculated per-time-series-key cardinality.
   * I.e. the number of unique
   * tag key and value combinations.
   */
  protected int perKeyCardinality;
  
  /** How much data to scan for in each call. */
  protected NumberGenerator scanlength;
  
  /** A generator used to select a random time series key per read/scan. */
  protected NumberGenerator keychooser;
  
  /** A generator to select what operation to perform during the run phase. */
  protected DiscreteGenerator operationchooser;
  
  /** The maximum number of interval offsets from the starting timestamp. Calculated
   * based on the number of records configured for the run. */
  protected int maxOffsets;
  
  /** The number of records or operations to perform for this run. */
  protected int recordcount;
  
  /** The number of tag pairs per time series. */
  protected int tagPairs;
  
  /** The table we'll write to. */
  protected String table;
  
  /** How many time series keys will be generated. */
  protected int numKeys;
  
  /** The generated list of possible time series key values. */
  protected String[] keys;

  /** The generated list of possible tag key values. */
  protected String[] tagKeys;
  
  /** The generated list of possible tag value values. */
  protected String[] tagValues;
  
  /** The cardinality for each tag key. */
  protected int[] tagCardinality;

  /** The method by which to distribute writes over the available threads.
   *  Valid values: "key", "timeseries"
   * */
  protected String threadedWriteDistribution;

  /** The cardinality for each key and tag key combined. */
  protected int[] keyAndTagCardinality;
  
  /** A helper to skip non-incrementing tag values. */
  protected int firstIncrementableCardinality;
  
  /** How sparse the data written should be. */
  protected double sparsity;
  
  /** The percentage of time series that should be delayed in writes. */
  protected double delayedSeries;
  
  /** The maximum number of intervals to delay a series. */
  protected int delayedIntervals;
  
  /** The delimiter for tag pairs in fields. */
  protected String tagPairDelimiter;
  
  /** The delimiter between parameters for the delete key. */
  protected String deleteDelimiter;

  /** Whether or not to delete by whole tag (regardless of value). */
  protected boolean deleteByTag;

  /** The tag keys to delete by (if deleting by tag). */
  protected boolean[] deleteTags;
  
  /** Optional query time interval during deletes. */
  protected int deleteQueryTimeSpan;

  /** Whether or not the actual interval should be randomly chosen, using 
   * deleteQueryTimeSpan as the maximum value. */
  protected boolean deleteQueryRandomTimeSpan;
  
  /** The delimiter between timestamps for query time spans. */
  protected String queryTimeSpanDelimiter;

  /** The delimiter between tags in the list of tags to group by when doing GROUP BY. */
  protected String groupByTagsListDelimiter;
  
  /** The key used to store the group-by function for READ/SCAN queries in the standard YCSB data structure. */
  protected String groupByFunctionKey;

  /** The key used to store the group-by tag keys for READ/SCAN queries in the standard YCSB data structure. */
  protected String groupByTagsKey;

  /** The (optional) specification for which tags should have which values in READ queries. */
  protected int[] readTags;

  /** Whether or not to issue group-by with READ queries. */
  protected boolean readGroupBy;

  /** The function used for group-by's in READ queries. */
  protected String readGroupByFunction;
  
  /** The tag keys to group on in a READ query. */
  protected boolean[] readGroupBys;

  /** The (optional) specification for which tags should have which values in SCAN queries. */
  protected int[][] scanTags;

  /** The (optional) starting timestamp for SCAN queries. */
  protected int[] scanStartTs;

  /** Optional query time interval during scans. */
  protected int[] scanQueryTimeSpan;
  
  /** Whether or not the actual interval should be randomly chosen, using 
   * scanQueryTimeSpan as the maximum value. */
  protected boolean[] scanQueryRandomTimeSpan;
  
  /** Whether or not to issue group-by with SCAN queries. */
  protected boolean[] scanGroupBy;
  
  /** The function used for group-by's in SCAN queries. */
  protected String[] scanGroupByFunction;
  
  /** The tag keys to group on in a SCAN queries. */
  protected boolean[][] scanGroupBys;
  
  /** Whether or not to issue downsampling with SCAN queries. */
  protected boolean[] scanDownsample;
  
  /** The key used for storing the downsampling tag keys for SCAN queries in the standard YCSB data structure. */
  protected String scanDownsampleKey;
  
  /** The downsampling function used with SCAN queries. */
  protected String[] scanDownsampleFunction;
  
  /** The downsampling interval used with SCAN queries. */
  protected int[] scanDownsampleInterval;

  /**
   * Set to true if want to check correctness of reads. Must also
   * be set to true during loading phase to function.
   */
  protected boolean dataintegrity;
  
  /** Measurements to write data integrity results to. */
  protected Measurements measurements = Measurements.getMeasurements();

  /** YCSB-parameters. */
  protected boolean debug;
  
  @Override
  public void init(final Properties p) throws WorkloadException {
    properties = p;
    debug = Boolean.parseBoolean(
      p.getProperty(
        DEBUG_PROPERTY,
        p.getProperty(
          TIMESERIESWORKLOAD_DEBUG_PROPERTY,
          DEBUG_PROPERTY_DEFAULT)));
    if (debug) {
      System.out.println("[TimeSeriesWorkload.java] Properties: " + p);
    }
    doTransactions = Boolean.parseBoolean(p.getProperty(Client.DO_TRANSACTIONS_PROPERTY, "false"));
    recordcount =
        Integer.parseInt(p.getProperty(Client.RECORD_COUNT_PROPERTY, 
            Client.DEFAULT_RECORD_COUNT));
    if (recordcount == 0) {
      recordcount = Integer.MAX_VALUE;
    }
    timestampKey = p.getProperty(TIMESTAMP_KEY_PROPERTY, TIMESTAMP_KEY_PROPERTY_DEFAULT);
    valueKey = p.getProperty(VALUE_KEY_PROPERTY, VALUE_KEY_PROPERTY_DEFAULT);
    operationchooser = createOperationGenerator(properties);
    
    
    randomizeTimestampOrder = Boolean.parseBoolean(p.getProperty(
        RANDOMIZE_TIMESTAMP_ORDER_PROPERTY, 
        RANDOMIZE_TIMESTAMP_ORDER_PROPERTY_DEFAULT));
    randomizeTimeseriesOrder = Boolean.parseBoolean(p.getProperty(
        RANDOMIZE_TIMESERIES_ORDER_PROPERTY, 
        RANDOMIZE_TIMESERIES_ORDER_PROPERTY_DEFAULT));
    
    // setup the cardinality
    numKeys = Integer.parseInt(p.getProperty(CoreWorkload.FIELD_COUNT_PROPERTY, 
        CoreWorkload.FIELD_COUNT_PROPERTY_DEFAULT));
    tagPairs = Integer.parseInt(p.getProperty(TAG_COUNT_PROPERTY, 
        TAG_COUNT_PROPERTY_DEFAULT));
    threadedWriteDistribution = p.getProperty(THREADED_WRITE_DISTRIBUTION_PROPERTY, 
        THREADED_WRITE_DISTRIBUTION_PROPERTY_DEFAULT);
    sparsity = Double.parseDouble(p.getProperty(SPARSITY_PROPERTY, SPARSITY_PROPERTY_DEFAULT));
    tagCardinality = new int[tagPairs];
    
    final String requestdistrib =
        p.getProperty(CoreWorkload.REQUEST_DISTRIBUTION_PROPERTY, 
            CoreWorkload.REQUEST_DISTRIBUTION_PROPERTY_DEFAULT);
    if (requestdistrib.compareTo("uniform") == 0) {
      keychooser = new UniformLongGenerator(0, numKeys - 1);
    } else if (requestdistrib.compareTo("sequential") == 0) {
      keychooser = new SequentialGenerator(0, numKeys - 1);
    } else if (requestdistrib.compareTo("zipfian") == 0) {
      keychooser = new ScrambledZipfianGenerator(0, numKeys - 1);
    //} else if (requestdistrib.compareTo("latest") == 0) {
    //  keychooser = new SkewedLatestGenerator(transactioninsertkeysequence);
    } else if (requestdistrib.equals("hotspot")) {
      double hotsetfraction =
          Double.parseDouble(p.getProperty(CoreWorkload.HOTSPOT_DATA_FRACTION, 
              CoreWorkload.HOTSPOT_DATA_FRACTION_DEFAULT));
      double hotopnfraction =
          Double.parseDouble(p.getProperty(CoreWorkload.HOTSPOT_OPN_FRACTION, 
              CoreWorkload.HOTSPOT_OPN_FRACTION_DEFAULT));
      keychooser = new HotspotIntegerGenerator(0, numKeys - 1,
          hotsetfraction, hotopnfraction);
    } else {
      throw new WorkloadException("Unknown request distribution \"" + requestdistrib + "\"");
    }
    
    // figure out the start timestamp based on the units, cardinality and interval
    try {
      timestampInterval = Integer.parseInt(p.getProperty(
          TIMESTAMP_INTERVAL_PROPERTY, TIMESTAMP_INTERVAL_PROPERTY_DEFAULT));
    } catch (NumberFormatException nfe) {
      throw new WorkloadException("Unable to parse the " + 
          TIMESTAMP_INTERVAL_PROPERTY, nfe);
    }
    
    try {
      timeUnits = TimeUnit.valueOf(p.getProperty(TIMESTAMP_UNITS_PROPERTY, 
          TIMESTAMP_UNITS_PROPERTY_DEFAULT).toUpperCase());
    } catch (IllegalArgumentException e) {
      throw new WorkloadException("Unknown time unit type", e);
    }
    if (timeUnits == TimeUnit.NANOSECONDS || timeUnits == TimeUnit.MICROSECONDS) {
      throw new WorkloadException("YCSB doesn't support " + timeUnits + 
          " at this time.");
    }
    
    tagPairDelimiter = p.getProperty(PAIR_DELIMITER_PROPERTY, PAIR_DELIMITER_PROPERTY_DEFAULT);
    deleteDelimiter = p.getProperty(DELETE_DELIMITER_PROPERTY, DELETE_DELIMITER_PROPERTY_DEFAULT);
    queryTimeSpanDelimiter = p.getProperty(QUERY_TIMESPAN_DELIMITER_PROPERTY, 
        QUERY_TIMESPAN_DELIMITER_PROPERTY_DEFAULT);
    groupByTagsListDelimiter = p.getProperty(GROUPBY_TAGS_LIST_DELIMITER_PROPERTY, 
        GROUPBY_TAGS_LIST_DELIMITER_PROPERTY_DEFAULT);
    dataintegrity = Boolean.parseBoolean(
        p.getProperty(CoreWorkload.DATA_INTEGRITY_PROPERTY, 
            CoreWorkload.DATA_INTEGRITY_PROPERTY_DEFAULT));

    // Need to init keys and tags here because they are used as part of setting up
    // the groupBy stuff below
    initKeysAndTags();

    // These queries seems to be redundant in the TimeSeries Context?
    // We have created our own concept of a SCAN, being a scan across
    // time (as oppsoed to keys), and therefore the "length" is the
    // query time span, which has its own property.
    // TODO: Check to see if these should either be supported (eg. scanning across
    // multiple time series keys at the same time as scanning in time), or 
    // whether these should be dropped
    final int maxscanlength =
        Integer.parseInt(p.getProperty(CoreWorkload.MAX_SCAN_LENGTH_PROPERTY, 
            CoreWorkload.MAX_SCAN_LENGTH_PROPERTY_DEFAULT));
    String scanlengthdistrib =
        p.getProperty(CoreWorkload.SCAN_LENGTH_DISTRIBUTION_PROPERTY, 
            CoreWorkload.SCAN_LENGTH_DISTRIBUTION_PROPERTY_DEFAULT);
    
    if (scanlengthdistrib.compareTo("uniform") == 0) {
      scanlength = new UniformLongGenerator(1, maxscanlength);
    } else if (scanlengthdistrib.compareTo("zipfian") == 0) {
      scanlength = new ZipfianGenerator(1, maxscanlength);
    } else {
      throw new WorkloadException(
          "Distribution \"" + scanlengthdistrib + "\" not allowed for scan length");
    }

    initScanQueryProperties();
    initReadQueryProperties();
    initDeleteQueryProperties();
   
    // These two properties are not individual-SCAN-query-specific, but rather apply for the whole
    // workload, therefore they are just pulled and stored once.
    groupByFunctionKey = p.getProperty(GROUPBY_FUNCTION_KEY_PROPERTY, GROUPBY_FUNCTION_KEY_PROPERTY_DEFAULT);
    groupByTagsKey = p.getProperty(GROUPBY_TAGS_KEY_PROPERTY, GROUPBY_TAGS_KEY_PROPERTY_DEFAULT);
    scanDownsampleKey = p.getProperty(SCAN_DOWNSAMPLING_KEY_PROPERTY, SCAN_DOWNSAMPLING_KEY_PROPERTY_DEFAULT);
    
    delayedSeries = Double.parseDouble(p.getProperty(DELAYED_SERIES_PROPERTY, DELAYED_SERIES_PROPERTY_DEFAULT));
    delayedIntervals = Integer.parseInt(p.getProperty(DELAYED_INTERVALS_PROPERTY, DELAYED_INTERVALS_PROPERTY_DEFAULT));
    
    valueType = ValueType.fromString(p.getProperty(VALUE_TYPE_PROPERTY, VALUE_TYPE_PROPERTY_DEFAULT));
    table = p.getProperty(CoreWorkload.TABLENAME_PROPERTY, CoreWorkload.TABLENAME_PROPERTY_DEFAULT);
    validateSettings();
  }
  
  @Override
  public Object initThread(Properties p, int mythreadid, int threadcount) throws WorkloadException {
    if (properties == null) {
      throw new WorkloadException("Workload has not been initialized.");
    }
    return new ThreadState(mythreadid, threadcount);
  }
  
  @Override
  public boolean doInsert(DB db, Object threadstate) {
    if (threadstate == null) {
      throw new IllegalStateException("Missing thread state.");
    }
    final Map<String, ByteIterator> tags = new TreeMap<String, ByteIterator>();
    final String key = ((ThreadState)threadstate).nextDataPoint(tags, true);
    if (db.insert(table, key, tags) == Status.OK) {
      return true;
    }
    return false;
  }

  @Override
  public boolean doTransaction(DB db, Object threadstate) {
    if (threadstate == null) {
      throw new IllegalStateException("Missing thread state.");
    }
    final String operation = operationchooser.nextString();
    switch (operation) {
    case "READ":
      doTransactionRead(db, threadstate);
      break;
    case "UPDATE":
      doTransactionUpdate(db, threadstate);
      break;
    case "INSERT": 
      doTransactionInsert(db, threadstate);
      break;
    case "SCAN":
      doTransactionScan(db, threadstate, 0);
      break;
    case "DELETE":
      doTransactionDelete(db, threadstate);
      break;
    default:
      if (operation.matches("SCAN[0-9]{0,3}")) {
        doTransactionScan(db, threadstate, new Integer(operation.replaceAll("SCAN", "")));
        break;
      } else {
        return false;
      }
    }
    return true;
  }

  /**
   * Creates a weighted discrete values with database operations for a workload to perform.
   * Weights/proportions are read from the properties list.
   *
   * **For a TimeSeriesWorkload NO Defaults are used***
   *
   * Looks through all properties for property names ending with "proportion" and adds
   * these to the operation generator with the proportion given in the property.
   * As operation name the part of the string before "proportion" will be upper-cased
   * and used. For example:
   *   readproportion=0.10 would result in an operation named "READ" being added with proportion 0.1
   *   scan1proportion=0.02 would result in sn operation named "SCAN1" being added with propotion 0.02
   *
   * Supported Operation Names are:
   * - READ
   * - INSERT
   * - UPDATE (though this may not be supported by the SUT)
   * - DELETE (though this may not be supported by the SUT)
   * - SCAN[n] (the n is optional but should be used when more than one SCAN query is part of the workload)
   *
   * This allows the a dynamic number of SCAN workloads to be defined in the properties, each
   * with their own proportion.
   *
   * Note: For *all* SCAN queries, it is dependent upon the existence of a companion properties, which
   *       should be named in the form scan[n][property]. So if you have scan1proportion=0.2 defined, then
   *       you must also define the various scan1[property] properties with the appropriate
   *       that define that query. For more details on which properties define a SCAN query, see
   *       the tsworkload_template file.
   *
   * @param p The properties list to pull weights from.
   * @return A generator that can be used to determine the next operation to perform.
   * @throws IllegalArgumentException if the properties object was null
   * @throws WorkloadException if the parsed operationName from a [operationName]proportion property is not one
   *         of the supported operations
   */
  protected static DiscreteGenerator createOperationGenerator(final Properties p) throws WorkloadException {
    if (p == null) {
      throw new IllegalArgumentException("Properties object cannot be null");
    }

    Set<String> proportionPropertyNames = p.stringPropertyNames().stream()
                                              .filter(prop -> prop.matches(".*proportion"))
                                              .collect(Collectors.toSet());

    final DiscreteGenerator operationchooser = new DiscreteGenerator();
    for (String prop : proportionPropertyNames) {
      String operationName = prop.replaceAll("proportion", "").toUpperCase();
      if (operationName.matches("READ|INSERT|UPDATE|DELETE|SCAN[0-9]{0,3}")) {
        double proportion = Double.parseDouble(p.getProperty(prop));
        if (proportion > 0) {
          operationchooser.addValue(proportion, operationName);
        }
      } else {
        throw new WorkloadException("Invalid Operation Name for Proportion Property: " + 
            prop + ", name: " + operationName + ", name must match pattern: " +
            "READ|INSERT|UPDATE|DELETE|SCAN[0-9]{0,3}");
      }
    }

    return operationchooser;
  }

  protected void doTransactionRead(final DB db, Object threadstate) {
    final ThreadState state = (ThreadState) threadstate;
    final String keyname = keys[keychooser.nextValue().intValue()];
    final Random random = ThreadLocalRandom.current();
    int offsets = state.queryOffsetGenerator.nextValue().intValue();
    //int offsets = random.nextInt(maxOffsets - 1);
    final long startTimestamp;
    if (offsets > 0) {
      startTimestamp = state.startTimestamp + state.timestampGenerator.getOffset(offsets);
    } else {
      startTimestamp = state.startTimestamp;
    }
    
    // Handle tag value assignment based on configuration
    Set<String> fields = new HashSet<String>();
    for (int i = 0; i < tagPairs; ++i) {
      // First priority is if grouping by tag is enabled for this tag key
      // If so, then we just add the tag key to the fields with no value,
      // which implies that all values for this tag should be selected,
      // and then grouped
      if (readGroupBy && readGroupBys[i]) {
        fields.add(tagKeys[i]);
      // Second priority is via the tag specification - if it is a negative number
      // then this means that the tag should not be specified, in order to allow
      // multiple time series associated with different tags to be grouped together
      } else if (readTags != null && readTags[i] < 0) {
        fields.add(tagKeys[i]);
      // Third priority is also via the tag specification - if it has been
      // specified to have a specific tag value.
      // If the value in readTags[i] is zero, then this says
      // the value should be random, so it then goes to the final else statement
      // otherwise we pull the value based on the specified index.
      // NOTE: the index specified in the read[n]tags config is 1-based (because 0
      // has a special meaning), but obviously the array of tagValues is 0-indexed,
      // so we need to adjust for that!
      } else if (readTags != null && readTags[i] != 0) {
        fields.add(tagKeys[i] + tagPairDelimiter + 
            tagValues[readTags[i] - 1]);
      } else { // Assign random value from the set of possible values
        fields.add(tagKeys[i] + tagPairDelimiter + 
            tagValues[random.nextInt(tagCardinality[i])]);
      }
    }
    
    fields.add(timestampKey + tagPairDelimiter + startTimestamp);

    if (readGroupBy) {
      // First add the function to the fields with the special key for the group by function
      fields.add(groupByFunctionKey + tagPairDelimiter + readGroupByFunction);
      // Then add the list of tags, separated by the delimiter defined in the config,
      // with the special key for group by tags
      fields.add(groupByTagsKey +
          tagPairDelimiter +
          IntStream.range(0, readGroupBys.length)
          .filter(i -> readGroupBys[i])
          .mapToObj(i -> tagKeys[i])
          .collect(Collectors.joining(groupByTagsListDelimiter)));
    }
    
    final Map<String, ByteIterator> cells = new HashMap<String, ByteIterator>();
    final Status status = db.read(table, keyname, fields, cells);
    
    if (dataintegrity && status == Status.OK) {
      verifyRow(keyname, cells);
    }
  }
  
  protected void doTransactionUpdate(final DB db, Object threadstate) {
    if (threadstate == null) {
      throw new IllegalStateException("Missing thread state.");
    }
    final Map<String, ByteIterator> tags = new TreeMap<String, ByteIterator>();
    final String key = ((ThreadState)threadstate).nextDataPoint(tags, false);
    db.update(table, key, tags);
  }
  
  protected void doTransactionInsert(final DB db, Object threadstate) {
    doInsert(db, threadstate);
  }
  
  protected void doTransactionScan(final DB db, Object threadstate, Integer scanQueryIndex) {
    if (debug) {
      System.out.println("[TimeSeriesWorkload.java] doTransactionScan() with scanQueryIndex=" + scanQueryIndex);
    }
    final ThreadState state = (ThreadState) threadstate;
    final Random random = ThreadLocalRandom.current();
    final String keyname = keys[random.nextInt(keys.length)];
   
    // This doesn't appear to be used in the clients, but it's
    // passed anyway because it's part of the original YCSB data structure.
    // TODO: See if this is actually needed
    // choose a random scan length
    int len = scanlength.nextValue().intValue();
   
    final long startTimestamp;
    if (scanStartTs[scanQueryIndex] != 0) { // a specifically-configured start timestamp is present
      if (scanStartTs[scanQueryIndex] < 0) { // we have a negative offset from latest-written timestamp
        // plus cos it's a negative value
        startTimestamp = state.timestampGenerator.currentValue() + scanStartTs[scanQueryIndex];
      } else { // we have a specific, fixed start timestamp
        startTimestamp = scanStartTs[scanQueryIndex];
      }
    } else { // no configured timestamp, choose randomly between startTimestamp and current max timestamp
      int offsets;
      // Check and see how far we are with writing (assuming writing is happening during run phase)
      final long currentWriteDeltaInOffsets = (state.timestampGenerator.currentValue() -
          state.startTimestamp) / timestampInterval;
      if (currentWriteDeltaInOffsets > maxOffsets - 1) { // We are writing during run phase
        offsets = random.nextInt((int) currentWriteDeltaInOffsets);
      } else { // We are not writing, so max timestamp is based on original maxOffsets
        offsets = random.nextInt(maxOffsets - 1);
      }
      if (offsets > 0) {
        startTimestamp = state.startTimestamp + state.timestampGenerator.getOffset(offsets);
      } else {
        startTimestamp = state.startTimestamp;
      }
    }
    
    // Handle tag value assignment based on configuration
    Set<String> fields = new HashSet<String>();
    for (int i = 0; i < tagPairs; ++i) {
      // First priority is if grouping by tag is enabled for this tag key
      // If so, then we just add the tag key to the fields with no value,
      // which implies that all values for this tag should be selected,
      // and then grouped
      if (scanGroupBy[scanQueryIndex] && scanGroupBys[scanQueryIndex][i]) {
        fields.add(tagKeys[i]);
      // Second priority is via the tag specification - if it is a negative number
      // then this means that the tag should not be specified, in order to allow
      // multiple time series associated with different tags to be grouped together
      } else if (scanTags[scanQueryIndex] != null && scanTags[scanQueryIndex][i] < 0) {
        fields.add(tagKeys[i]);
      // Third priority is also via the tag specification - if it has been
      // specified to have a specific tag value.
      // If the value in scanTags[scanQueryIndex][i] is zero, then this says
      // the value should be random, so it then goes to the final else statement
      // otherwise we pull the value based on the specified index.
      // NOTE: the index specified in the scan[n]tags config is 1-based (because 0
      // has a special meaning), but obviously the array of tagValues is 0-indexed,
      // so we need to adjust for that!
      } else if (scanTags[scanQueryIndex] != null && scanTags[scanQueryIndex][i] != 0) {
        fields.add(tagKeys[i] + tagPairDelimiter + 
            tagValues[scanTags[scanQueryIndex][i] - 1]);
      } else { // Assign random value from the set of possible values
        fields.add(tagKeys[i] + tagPairDelimiter + 
            tagValues[random.nextInt(tagCardinality[i])]);
      }
    }
    
    if (scanQueryTimeSpan[scanQueryIndex] > 0) { // endTimestamp is different to startTimestamp
      final long endTimestamp;
      if (scanQueryRandomTimeSpan[scanQueryIndex]) {
        endTimestamp = startTimestamp +
          (timestampInterval * random.nextInt(scanQueryTimeSpan[scanQueryIndex] / timestampInterval));
      } else {
        endTimestamp = startTimestamp + scanQueryTimeSpan[scanQueryIndex];
      }
      fields.add(timestampKey + tagPairDelimiter + startTimestamp + queryTimeSpanDelimiter + endTimestamp);
    } else { // otherwise just set the endTimestamp to be the startTimestamp
      fields.add(timestampKey + tagPairDelimiter + startTimestamp + queryTimeSpanDelimiter + startTimestamp);  
    }
    if (scanGroupBy[scanQueryIndex]) {
      // First add the function to the fields with the special key for the group by function
      fields.add(groupByFunctionKey + tagPairDelimiter + scanGroupByFunction[scanQueryIndex]);
      // Then add the list of tags, separated by the delimiter defined in the config,
      // with the special key for group by tags
      fields.add(groupByTagsKey +
          tagPairDelimiter +
          IntStream.range(0, scanGroupBys[scanQueryIndex].length)
          .filter(i -> scanGroupBys[scanQueryIndex][i])
          .mapToObj(i -> tagKeys[i])
          .collect(Collectors.joining(groupByTagsListDelimiter)));
    }
    if (scanDownsample[scanQueryIndex]) {
      fields.add(scanDownsampleKey +
          tagPairDelimiter +
          scanDownsampleFunction[scanQueryIndex] +
          scanDownsampleInterval[scanQueryIndex]);
    }
    
    final Vector<HashMap<String, ByteIterator>> results = new Vector<HashMap<String, ByteIterator>>();
    long startTime = System.nanoTime();
    Status returnStatus = db.scan(table, keyname, len, fields, results);
    long endTime = System.nanoTime();
    if (scanQueryIndex != 0) {
      // Note: YCSB expect latency measurements in microseconds it seems
      // See: https://github.com/brianfrankcooper/YCSB/blob
      // /cd1589ce6f5abf96e17aa8ab80c78a4348fdf29a/core/src/main/java/site/ycsb/DBWrapper.java#L179
      Measurements.getMeasurements().measure("SCAN" + scanQueryIndex,  (int)((endTime - startTime)/1000));
      // Report status for each individual SCAN query so that's also reported in the results
      Measurements.getMeasurements().reportStatus("SCAN" + scanQueryIndex, returnStatus);
    }
  }
  
  protected void doTransactionDelete(final DB db, Object threadstate) {
    final ThreadState state = (ThreadState) threadstate;
    final Random random = ThreadLocalRandom.current();
    final StringBuilder buf = new StringBuilder().append(keys[random.nextInt(keys.length)]);
    
    int offsets = random.nextInt(maxOffsets - 1);
    final long startTimestamp;
    if (offsets > 0) {
      startTimestamp = state.startTimestamp + state.timestampGenerator.getOffset(offsets);
    } else {
      startTimestamp = state.startTimestamp;
    }
    
    // rando tags
    for (int i = 0; i < tagPairs; ++i) {
      // Append only the tag key if we are deleting multiple time series by key
      if (deleteByTag && deleteTags[i]) {
        buf.append(deleteDelimiter)
           .append(tagKeys[i]);
      } else { // otherwise append full key-value pairs for selecting time series to be deleted
        buf.append(deleteDelimiter).append(tagKeys[i] + tagPairDelimiter + 
            tagValues[random.nextInt(tagCardinality[i])]);
      }
    }
    
    if (deleteQueryTimeSpan > 0) {
      final long endTimestamp;
      if (deleteQueryRandomTimeSpan) {
        endTimestamp = startTimestamp + (timestampInterval * random.nextInt(deleteQueryTimeSpan / timestampInterval));
      } else {
        endTimestamp = startTimestamp + deleteQueryTimeSpan;
      }
      buf.append(deleteDelimiter)
         .append(timestampKey + tagPairDelimiter + startTimestamp + queryTimeSpanDelimiter + endTimestamp);
    } else {
      buf.append(deleteDelimiter)
         .append(timestampKey + tagPairDelimiter + startTimestamp);  
    }
    
    db.delete(table, buf.toString());
  }
  
  /**
   * Parses the values returned by a read or scan operation and determines whether
   * or not the integer value matches the hash and timestamp of the original timestamp.
   * Only works for raw data points, will not work for group-by's or downsampled data.
   * @param key The time series key.
   * @param cells The cells read by the DB.
   * @return {@link Status#OK} if the data matched or {@link Status#UNEXPECTED_STATE} if
   * the data did not match.
   */
  protected Status verifyRow(final String key, final Map<String, ByteIterator> cells) {
    Status verifyStatus = Status.UNEXPECTED_STATE;
    long startTime = System.nanoTime();

    double value = 0;
    long timestamp = 0;
    final TreeMap<String, String> validationTags = new TreeMap<String, String>();
    for (final Entry<String, ByteIterator> entry : cells.entrySet()) {
      if (entry.getKey().equals(timestampKey)) {
        final NumericByteIterator it = (NumericByteIterator) entry.getValue();
        timestamp = it.getLong();
      } else if (entry.getKey().equals(valueKey)) {
        final NumericByteIterator it = (NumericByteIterator) entry.getValue();
        value = it.isFloatingPoint() ? it.getDouble() : it.getLong();
      } else {
        validationTags.put(entry.getKey(), entry.getValue().toString());
      }
    }

    if (validationFunction(key, timestamp, validationTags) == value) {
      verifyStatus = Status.OK;
    }
    long endTime = System.nanoTime();
    measurements.measure("VERIFY", (int) (endTime - startTime) / 1000);
    measurements.reportStatus("VERIFY", verifyStatus);
    return verifyStatus;
  }
  
  /**
   * Function used for generating a deterministic hash based on the combination
   * of metric, tags and timestamp.
   * @param key A non-null string representing the key.
   * @param timestamp A timestamp in the proper units for the workload.
   * @param tags A non-null map of tag keys and values NOT including the YCSB
   * key or timestamp.
   * @return A hash value as an 8 byte integer.
   */
  protected long validationFunction(final String key, final long timestamp, 
                                    final TreeMap<String, String> tags) {
    final StringBuilder validationBuffer = new StringBuilder(keys[0].length() + 
        (tagPairs * tagKeys[0].length()) + (tagPairs * tagCardinality[1]));
    for (final Entry<String, String> pair : tags.entrySet()) {
      validationBuffer.append(pair.getKey()).append(pair.getValue());
    }
    return (long) validationBuffer.toString().hashCode() ^ timestamp;
  }
  
  /**
   * Breaks out the keys, tags and cardinality initialization in another method
   * to keep CheckStyle happy.
   * @throws WorkloadException If something goes pear shaped.
   */
  protected void initKeysAndTags() throws WorkloadException {
    final int keyLength = Integer.parseInt(properties.getProperty(
        CoreWorkload.FIELD_LENGTH_PROPERTY, 
        CoreWorkload.FIELD_LENGTH_PROPERTY_DEFAULT));
    final int tagKeyLength = Integer.parseInt(properties.getProperty(
        TAG_KEY_LENGTH_PROPERTY, TAG_KEY_LENGTH_PROPERTY_DEFAULT));
    final int tagValueLength = Integer.parseInt(properties.getProperty(
        TAG_VALUE_LENGTH_PROPERTY, TAG_VALUE_LENGTH_PROPERTY_DEFAULT));
    
    keyGenerator = new IncrementingPrintableStringGenerator(keyLength);
    tagKeyGenerator = new IncrementingPrintableStringGenerator(tagKeyLength);
    tagValueGenerator = new IncrementingPrintableStringGenerator(tagValueLength);
    
    final int threads = Integer.parseInt(properties.getProperty(Client.THREAD_COUNT_PROPERTY, "1"));
    final String tagCardinalityString = properties.getProperty(
        TAG_CARDINALITY_PROPERTY, 
        TAG_CARDINALITY_PROPERTY_DEFAULT);
    final String[] tagCardinalityParts = tagCardinalityString.split(",");
    int idx = 0;
    totalCardinality = numKeys;
    perKeyCardinality = 1;
    int maxCardinality = 0; // represents the highest individual tag cardinality
    for (final String card : tagCardinalityParts) {
      try {
        tagCardinality[idx] = Integer.parseInt(card.trim());
      } catch (NumberFormatException nfe) {
        throw new WorkloadException("Unable to parse cardinality: " + 
            card, nfe);
      }
      if (tagCardinality[idx] < 1) {
        throw new WorkloadException("Cardinality must be greater than zero: " + 
            tagCardinality[idx]);
      }
      totalCardinality *= tagCardinality[idx];
      perKeyCardinality *= tagCardinality[idx];
      if (tagCardinality[idx] > maxCardinality) {
        maxCardinality = tagCardinality[idx];
      }
      ++idx;
      if (idx >= tagPairs) {
        // we have more cardinalities than tag keys so bail at this point.
        break;
      }
    }
    if (threadedWriteDistribution == "key" && numKeys < threads) {
      throw new WorkloadException("Field count " + numKeys + " (keys for time "
          + "series workloads) must be greater or equal to the number of "
          + "threads " + threads);
    } else if (totalCardinality < threads) {
      throw new WorkloadException("total cardinality " + totalCardinality + " (combination "
          + "of key & tag cardinality) must be greater or equal to the number of "
          + "threads " + threads);
    }
    
    // fill tags without explicit cardinality with 1
    if (idx < tagPairs) {
      tagCardinality[idx++] = 1;
    }
    
    for (int i = 0; i < tagCardinality.length; ++i) {
      if (tagCardinality[i] > 1) {
        firstIncrementableCardinality = i;
        break;
      }
    }
    
    keys = new String[numKeys];
    tagKeys = new String[tagPairs];
    tagValues = new String[maxCardinality];
    for (int i = 0; i < numKeys; ++i) {
      keys[i] = keyGenerator.nextString();
    }

    for (int i = 0; i < tagPairs; ++i) {
      tagKeys[i] = tagKeyGenerator.nextString();
    }
    
    for (int i = 0; i < maxCardinality; i++) {
      tagValues[i] = tagValueGenerator.nextString();
    }
    if (randomizeTimeseriesOrder) {
      Utils.shuffleArray(keys);
      Utils.shuffleArray(tagValues);
    }
    
    maxOffsets = (recordcount / totalCardinality) + 1;
    keyAndTagCardinality = new int[tagPairs + 1];
    keyAndTagCardinality[0] = numKeys;
    for (int i = 0; i < tagPairs; i++) {
      keyAndTagCardinality[i + 1] = tagCardinality[i];
    }
    
    cumulativeCardinality = new int[keyAndTagCardinality.length];
    for (int i = 0; i < keyAndTagCardinality.length; i++) {
      int cumulation = 1;
      for (int x = i; x <= keyAndTagCardinality.length - 1; x++) {
        cumulation *= keyAndTagCardinality[x];
      }
      if (i > 0) {
        cumulativeCardinality[i - 1] = cumulation;
      }
    }
    cumulativeCardinality[cumulativeCardinality.length - 1] = 1;

    // Is an array that hold the cumulative total cardinality
    // including the key and all tags, for each step from key
    // through to the last tag.
    // Eg. if numKeys = 2, and tagCardinality = [2,3,2]
    // Then cumulativeTotalCardinality = [2, 4, 12, 24]
    cumulativeTotalCardinality = new int[keyAndTagCardinality.length];
    for (int i = 0; i < keyAndTagCardinality.length; i++) {
      int cumulation = 1;
      for (int j = 0; j <= i; j++) {
        cumulation *= keyAndTagCardinality[j];
      }
      cumulativeTotalCardinality[i] = cumulation;
    }
    if (debug) {
      System.out.println("[TimeSeriesWorkload.java][initKeysAndTags] totalCardinality = " + totalCardinality);
      System.out.println("[TimeSeriesWorkload.java][initKeysAndTags] keyAndTagCardinality = " +
          Arrays.toString(keyAndTagCardinality));
      System.out.println("[TimeSeriesWorkload.java][initKeysAndTags] cumulativeTotalCardinality = " +
          Arrays.toString(cumulativeTotalCardinality));
    }
  }

  /**
   * Breaks out the delete query property initialization in another method
   * to keep CheckStyle happy.
   * @throws WorkloadException If something goes pear shaped.
   */
  protected void initDeleteQueryProperties() throws WorkloadException {
    deleteQueryTimeSpan = Integer.parseInt(properties.getProperty(DELETE_QUERY_TIMESPAN_PROPERTY, 
        DELETE_QUERY_TIMESPAN_PROPERTY_DEFAULT));
    deleteQueryRandomTimeSpan = Boolean.parseBoolean(properties.getProperty(DELETE_QUERY_RANDOM_TIMESPAN_PROPERTY, 
        DELETE_QUERY_RANDOM_TIMESPAN_PROPERTY_DEFAULT));

    // Delete by Tag Keys only (instead of fully-defined tag key-value pairs)
    final String deleteTagKeys = properties.getProperty(DELETE_TAG_KEYS_PROPERTY);
    if (deleteTagKeys != null && !deleteTagKeys.isEmpty()) {
      final String[] delTagKeys = deleteTagKeys.split(",");
      if (delTagKeys.length != tagKeys.length) {
        throw new WorkloadException("Only " + delTagKeys.length + " delete tag keys "
            + "were specified but there were " + tagKeys.length + " tag keys given.");
      }
      deleteTags = new boolean[delTagKeys.length];
      for (int i = 0; i < delTagKeys.length; i++) {
        deleteTags[i] = Integer.parseInt(delTagKeys[i].trim()) == 0 ? false : true;
      }
      deleteByTag = true;
    }
  }

  /**
   * Breaks out the read query property initialization in another method
   * to keep CheckStyle happy.
   * @throws WorkloadException If something goes pear shaped.
   */
  protected void initReadQueryProperties() throws WorkloadException {
    // See READ_TAGS_PROPERTY in tsworkload_template for explanation
    final String readTagsPropertyValue = properties.getProperty(READ_TAGS_PROPERTY);
    if (readTagsPropertyValue != null && !readTagsPropertyValue.isEmpty()) {
      final String[] readTagsValues = readTagsPropertyValue.split(",");
      if (readTagsValues.length != tagKeys.length) {
        throw new WorkloadException("READ: Only " + readTagsValues.length + " readtags values"
            + "were specified but there were " + tagKeys.length + " tag keys given.");
      }
      readTags = new int[readTagsValues.length];
      for (int i = 0; i < readTagsValues.length; i++) {
        final int tagKeyValueIndex = Integer.parseInt(readTagsValues[i].trim());
        if (tagKeyValueIndex > tagCardinality[i]) {
          throw new WorkloadException("readtags: value given for tag at index " + i + " ("
              + tagKeyValueIndex + ") is larger than the tag cardinality for that tag (" + tagCardinality[i] + ")");
        }
        readTags[i] = Integer.parseInt(readTagsValues[i].trim());
      }
    }

    readGroupByFunction = properties.getProperty(READ_GROUPBY_PROPERTY);
    if (readGroupByFunction != null && !readGroupByFunction.isEmpty()) {
      final String readGroupByTagKeys = properties.getProperty(READ_GROUPBY_TAG_KEYS_PROPERTY);
      if (readGroupByTagKeys == null || readGroupByTagKeys.isEmpty()) {
        throw new WorkloadException("READ Group by was enabled but no keys were specified.");
      }
      final String[] readGbKeys = readGroupByTagKeys.split(",");
      if (readGbKeys.length != tagKeys.length) {
        throw new WorkloadException("Only " + readGbKeys.length + " READ group by keys "
            + "were specified but there were " + tagKeys.length + " tag keys given.");
      }
      readGroupBys = new boolean[readGbKeys.length];
      for (int i = 0; i < readGbKeys.length; i++) {
        readGroupBys[i] = Integer.parseInt(readGbKeys[i].trim()) == 0 ? false : true;
      }
      readGroupBy = true;
    }
  }

  /**
   * Breaks out the scan query/queries property initialization in another method
   * to keep CheckStyle happy.
   * @throws WorkloadException If something goes pear shaped.
   */
  protected void initScanQueryProperties() throws WorkloadException {
    // Parse all the properties for the SCAN query/queries (if there's more than one)
    // Each variable for the SCAN query properties is an array, to allow for
    // the possiblity to have multiple SCAN queries in one workload.
    // If scan properties were entered for just one scan query, then all the 
    // relevant properties just begin with the prefix 'scan'. These properties
    // will be stores at the 0 index of the arrays that store all the SCAN query
    // properties.
    // For multiple SCAN queries in one workload, it is expected that the prefix
    // includes and index, eg. scan1, scan2, etc.
    // This index will also be the array index used. This means it's possible that
    // someone only uses scan properties with indexes, but this is also okay, because
    // the operationChooser also has one operation for each query, with the index,
    // and this index will be passed to the method that performs the query, which
    // will in turn pull the values out of the arrays based on the given index. So
    // if there's no properties stored at the zero index, this is okay.
    // Some SCAN properties are required and/or have default values, and we will
    // pull them from the relevant default defined towards the tops of this file.

    // We retrieve the "list" of SCAN queries defined in the properties by looking
    // for all the scan[n]proportion properties defined, because the query cannot
    // exist without a proportion. We then transform these into indexes by either
    // pulling the index directly out of the property name or setting it to zero if
    // no index number was present.
    Set<Integer> scanQueryIndexes = properties.stringPropertyNames().stream()
                                        .filter(prop -> prop.matches("scan[0-9]{0,3}proportion"))
                                        .map(scanProp -> {
                                            if (scanProp.matches(".*\\d.*")) {
                                              return new Integer(scanProp.replaceAll("[^0-9]", ""));
                                            } else {
                                              return new Integer(0);
                                            }
                                          }).collect(Collectors.toSet());

    int scanQueryMaxIndex = Collections.max(scanQueryIndexes);
    if (debug) {
      System.out.println("[TimeSeriesWorkload.java] scanQueryIndexes found: " + scanQueryIndexes);
      System.out.println("[TimeSeriesWorkload.java] scanQueryMaxIndex: " + scanQueryMaxIndex);
    }

    // Initialize all SCAN property arrays based on highest index found
    // Note: this might mean that the array is larger than the number of indexes, but then
    // we ensure that the scan query index should always match the array index.
    scanTags = new int[scanQueryMaxIndex + 1][];
    scanStartTs = new int[scanQueryMaxIndex + 1];
    scanQueryTimeSpan = new int[scanQueryMaxIndex + 1];
    scanQueryRandomTimeSpan = new boolean[scanQueryMaxIndex + 1];
    scanGroupByFunction = new String[scanQueryMaxIndex + 1];
    scanGroupBys = new boolean[scanQueryMaxIndex + 1][];
    scanGroupBy = new boolean[scanQueryMaxIndex + 1];
    scanDownsampleFunction = new String[scanQueryMaxIndex + 1];
    scanDownsampleInterval = new int[scanQueryMaxIndex + 1];
    scanDownsample = new boolean[scanQueryMaxIndex + 1];

    // Now loop through each index, retrieve the properties and assign to the arrays
    for (Integer scanQueryIndex: scanQueryIndexes) {
      if (debug) {
        System.out.println("[TimeSeriesWorkload.java] Parsing SCAN Query Properties, for Index: " + scanQueryIndex);
      }

      // See SCAN_TAGS_PROPERTY in tsworkload_template for explanation
      final String scanTagsPropertyValue = properties.getProperty(
          indexedScanProperty(scanQueryIndex, SCAN_TAGS_PROPERTY));
      if (scanTagsPropertyValue != null && !scanTagsPropertyValue.isEmpty()) {
        final String[] scanTagsValues = scanTagsPropertyValue.split(",");
        if (scanTagsValues.length != tagKeys.length) {
          throw new WorkloadException("SCAN" + scanQueryIndex + ": Only " + scanTagsValues.length + " scantags values"
              + "were specified but there were " + tagKeys.length + " tag keys given.");
        }
        scanTags[scanQueryIndex] = new int[scanTagsValues.length];
        for (int i = 0; i < scanTagsValues.length; i++) {
          final int tagKeyValueIndex = Integer.parseInt(scanTagsValues[i].trim());
          if (tagKeyValueIndex > tagCardinality[i]) {
            throw new WorkloadException("scan" + scanQueryIndex + "tags: value given for tag at index " + i + " ("
                + tagKeyValueIndex + ") is larger than the tag cardinality for that tag (" + tagCardinality[i] + ")");
          }
          scanTags[scanQueryIndex][i] = Integer.parseInt(scanTagsValues[i].trim());
        }
      }

      // See SCAN_START_TS_PROPERTY property in tsworkload_template for full explanation.
      // Can either be a positive integer representing a specific timestamp, or a negative integer,
      // which represents a negative offset from the most recently-written timestamproperties. But here
      // we just store the raw int, the actual behaviour will be handled in the doTransactionScan method
      String scanStartTsPropertyValue = properties.getProperty(
          indexedScanProperty(scanQueryIndex, SCAN_START_TS_PROPERTY));
      if (scanStartTsPropertyValue != null) {
        scanStartTs[scanQueryIndex] = Integer.parseInt(scanStartTsPropertyValue);
      }
    
      if (debug) {
        System.out.println("[TimeSeriesWorkload.java] scanQueryTimeSpan[] " +
            "(before assignment) -> " + Arrays.toString(scanQueryTimeSpan));
      }
      // See SCAN_QUERY_TIMESPAN_PROPERTY property in tsworkload_template for full explanation.
      scanQueryTimeSpan[scanQueryIndex] = Integer.parseInt(properties.getProperty(
          indexedScanProperty(scanQueryIndex, SCAN_QUERY_TIMESPAN_PROPERTY), 
          SCAN_QUERY_TIMESPAN_PROPERTY_DEFAULT));
      if (debug) {
        System.out.println("[TimeSeriesWorkload.java] scanQueryTimeSpan[] " +
            "(after assignment) -> " + Arrays.toString(scanQueryTimeSpan));
      }

      // See SCAN_QUERY_RANDOM_TIMESPAN_PROPERTY property in tsworkload_template for full explanation.
      scanQueryRandomTimeSpan[scanQueryIndex] = Boolean.parseBoolean(properties.getProperty(
          indexedScanProperty(scanQueryIndex, SCAN_QUERY_RANDOM_TIMESPAN_PROPERTY), 
          SCAN_QUERY_RANDOM_TIMESPAN_PROPERTY_DEFAULT));

      // See SCAN_GROUPBY_PROPERTY property in tsworkload_template for full explanation.
      scanGroupByFunction[scanQueryIndex] = properties.getProperty(indexedScanProperty(scanQueryIndex,
            SCAN_GROUPBY_PROPERTY));
      if (scanGroupByFunction[scanQueryIndex] != null && !scanGroupByFunction[scanQueryIndex].isEmpty()) {
        final String scanGroupByTagKeys = properties.getProperty(indexedScanProperty(scanQueryIndex,
              SCAN_GROUPBY_TAG_KEYS_PROPERTY));
        if (scanGroupByTagKeys == null || scanGroupByTagKeys.isEmpty()) {
          throw new WorkloadException("SCAN" + scanQueryIndex + ": Group by was enabled but no keys were specified.");
        }
        final String[] scanGbKeys = scanGroupByTagKeys.split(",");
        if (scanGbKeys.length != tagKeys.length) {
          throw new WorkloadException("Only " + scanGbKeys.length + " SCAN" + scanQueryIndex + " group by keys "
              + "were specified but there were " + tagKeys.length + " tag keys given.");
        }
        scanGroupBys[scanQueryIndex] = new boolean[scanGbKeys.length];
        for (int i = 0; i < scanGbKeys.length; i++) {
          scanGroupBys[scanQueryIndex][i] = Integer.parseInt(scanGbKeys[i].trim()) == 0 ? false : true;
        }
        scanGroupBy[scanQueryIndex] = true;
      }
     
      // See SCAN_DOWNSAMPLING_FUNCTION_PROPERTY property in tsworkload_template for full explanation.
      scanDownsampleFunction[scanQueryIndex] = properties.getProperty(indexedScanProperty(scanQueryIndex,
            SCAN_DOWNSAMPLING_FUNCTION_PROPERTY));
      if (scanDownsampleFunction[scanQueryIndex] != null && !scanDownsampleFunction[scanQueryIndex].isEmpty()) {
        // See SCAN_DOWNSAMPLING_INTERVAL_PROPERTY property in tsworkload_template for full explanation.
        final String interval = properties.getProperty(
            indexedScanProperty(scanQueryIndex, SCAN_DOWNSAMPLING_INTERVAL_PROPERTY));
        if (interval == null || interval.isEmpty()) {
          throw new WorkloadException("SCAN" + scanQueryIndex
              + "'" + indexedScanProperty(scanQueryIndex, SCAN_DOWNSAMPLING_INTERVAL_PROPERTY)
              + "' was missing despite '" 
              + indexedScanProperty(scanQueryIndex, SCAN_DOWNSAMPLING_FUNCTION_PROPERTY)
              + "' being set.");
        }
        scanDownsampleInterval[scanQueryIndex] = Integer.parseInt(interval);
        scanDownsample[scanQueryIndex] = true;
      }
    }
  }
  
  /**
   * Makes sure the settings as given are compatible.
   * @throws WorkloadException If one or more settings were invalid.
   */
  protected void validateSettings() throws WorkloadException {
    if (dataintegrity) {
      if (valueType != ValueType.INTEGERS) {
        throw new WorkloadException("Data integrity was enabled. 'valuetype' must "
            + "be set to 'integers'.");
      }
      if (containsTrue(scanGroupBy)) {
        throw new WorkloadException("Data integrity was enabled. 'scan[n]groupbyfunction' must "
            + "be empty or null.");
      }
      if (containsTrue(scanDownsample)) {
        throw new WorkloadException("Data integrity was enabled. 'scan[n]downsamplingfunction' must "
            + "be empty or null.");
      }
      if (Arrays.stream(scanQueryTimeSpan).anyMatch(sQTS -> sQTS > 0)) {
        throw new WorkloadException("Data integrity was enabled. 'scan[n]querytimespan' must "
            + "be empty or 0.");
      }
      if (randomizeTimeseriesOrder) {
        throw new WorkloadException("Data integrity was enabled. 'randomizetimeseriesorder' must "
            + "be false.");
      }
      final String startTimestamp = properties.getProperty(CoreWorkload.INSERT_START_PROPERTY);
      if (startTimestamp == null || startTimestamp.isEmpty()) {
        throw new WorkloadException("Data integrity was enabled. 'insertstart' must "
            + "be set to a Unix Epoch timestamp.");
      }
    }
  }

  protected String indexedScanProperty(Integer index, String scanPropertyName) {
    if (index == 0) {
      return scanPropertyName;
    } else {
      return scanPropertyName.replaceFirst("scan", "scan" + index);
    }
  }

  protected boolean containsTrue(boolean[] booleanArray) {
    for(boolean element: booleanArray) {
      if (element) {
        return true;
      }
    }
    return false;
  }
  
  /**
   * Thread state class holding thread local generators and indices.
   */
  protected class ThreadState {
    /** The timestamp generator for this thread. */
    protected final UnixEpochTimestampGenerator timestampGenerator;

    /** An offset generator to select a random offset for queries. */
    protected final NumberGenerator queryOffsetGenerator;

    /** The current write time series index.
     *  This is the index of the key + tag combination.
     */
    protected int timeSeriesIndex;

    /** The starting index for writing time series in a thread. */
    protected int timeSeriesIndexStart;
    
    /** The upper times series index bound (exclusive) for
     * writing time series in a thread. */
    protected int timeSeriesIndexEnd;

    /**
     * The current key and tag indexes that represent
     * which time series is currently being written to
     * are stored in these arrays.
     *
     * If you take the cumulativeTotalCardinality and make a 
     * tree diagram out of it then the timeSeriesIndex is the
     * same as the index of the leaf at the edge of the tree, with
     * a range from 0 up to cumulativeCardinality[last].
     * This is stored in the last element of the keyAndTagTreeIndexes
     * array. Then the index at each level up is stored in the
     * earlier elements of the array.
     *
     * From these "full" tree indexes, we can generate the key
     * and tag value indexes by taking the modulo of the full
     * index with the cardinality for each key or tag. The
     * current value for these is stored in the keyAndTagIndexes array.
     *
     * */
    protected int[] keyAndTagTreeIndexes;
    protected int[] keyAndTagIndexes;
    
    /** The current write key index. */
    protected int keyIdx;
    
    /** The starting fence for writing keys. */
    protected int keyIdxStart;
    
    /** The ending fence for writing keys. */
    protected int keyIdxEnd;
    
    /** Indices for each tag value for writes. */
    protected int[] tagValueIdxs;

    /** Whether or not all time series have written values for the current timestamp. */
    protected boolean timestampRollover;
    
    /** The starting timestamp for Insert queries as part of a load run,
     *  or in a transation run it represents the starting point of the existing data
     *  that was previously loaded during the load run. */
    protected long startTimestamp;

    /** The starting timestamp for Read / Scan Queries. */
    protected long startTimestampRead;

    protected int threadIdentifier;
    
    /**
     * Default ctor.
     * @param threadID The zero based thread ID.
     * @param threadCount The total number of threads.
     * @throws WorkloadException If something went pear shaped.
     */
    protected ThreadState(final int threadID, final int threadCount) throws WorkloadException {
      int totalThreads = threadCount > 0 ? threadCount : 1;
      threadIdentifier = threadID;
      
      if (threadID >= totalThreads) {
        throw new IllegalStateException("Thread ID " + threadID + " cannot be greater "
            + "than or equal than the thread count " + totalThreads);
      }

      if (threadedWriteDistribution == "key" && keys.length < threadCount) {
        throw new WorkloadException("Thread count " + totalThreads + " must be greater "
            + "than or equal to key count " + keys.length);
      } else if (totalCardinality < threadCount) {
        throw new WorkloadException("Thread count " + totalThreads + " must be greater "
            + "than or equal to totalCardinality" + totalCardinality);
      }

      // An individual time series is represented by the
      // combination of the key + tags
      // We will split up the insert workload into groups
      // of time series and spread them across the threads
      // to parallelise.
      int timeSeriesPerThread = totalCardinality / totalThreads;
      timeSeriesIndex = timeSeriesPerThread * threadID;
      timeSeriesIndexStart = timeSeriesIndex;
      if (totalThreads - 1 == threadID) {
        timeSeriesIndexEnd = totalCardinality;
      } else {
        timeSeriesIndexEnd = timeSeriesIndexStart + timeSeriesPerThread;
      }

      keyAndTagTreeIndexes = new int[cumulativeTotalCardinality.length];
      keyAndTagIndexes = new int[cumulativeTotalCardinality.length];
      // Set initial values for the key and tag indexes
      for (int i = 0; i < keyAndTagCardinality.length; i++) {
        keyAndTagTreeIndexes[i] = timeSeriesIndex / (totalCardinality/ cumulativeTotalCardinality[i]);
        keyAndTagIndexes[i] = keyAndTagTreeIndexes[i] % keyAndTagCardinality[i];
      }
      
      int keysPerThread = keys.length / totalThreads;
      keyIdx = keysPerThread * threadID;
      keyIdxStart = keyIdx;
      if (totalThreads - 1 == threadID) {
        keyIdxEnd = keys.length;
      } else {
        // keyIdxEnd is an exclusive end
        // keyIdx is iterated from keyIdxStart..(keyIdxEnd-1)
        keyIdxEnd = keyIdxStart + keysPerThread; 
      }
      
      tagValueIdxs = new int[tagPairs]; // all zeros
     
      // startingTimestamp is the starting point for inserts on a load run
      final String startingTimestamp = 
          properties.getProperty(CoreWorkload.INSERT_START_PROPERTY);
      // startTimestamp is the thread property referenced during read, scan, and update queries as the reference for
      // what the starting timestamp is for existing data.
      // In order of priority, based on if the properties exist, it will end up with the value as follows:-
      //   1. value from READ_START_PROPERTY, if it exists
      //   2. value from INSERT_START_PROPERTY, if it exists
      //   3. first value from the insert timestampGenerator (see below)
      startTimestamp = Long.parseLong(properties.getProperty(READ_START_PROPERTY, startingTimestamp));
      // Insert-specific starting timestamp for initialising the timestamp generator which generates
      // the timestamps for inserts.
      // In a load run it's the same value as startingTimestamp,
      // but in a transaction run, it'll check for the presence of the INSERT_TRANSACTION_START_PROPERTY,
      // and if available will use that as the starting time stamp for inserts during the transaction run.
      // This allows first doing an insert run, which will be the basis data for the reads/scans in a following
      // transaction run, but for inserts during the transaction run, they will start
      // from the timestamp specified in the property.
      String startingInsertTimestamp = startingTimestamp;
      if (doTransactions) {
        startingInsertTimestamp = properties.getProperty(INSERT_TRANSACTION_START_PROPERTY, startingTimestamp);
      }
      if (startingInsertTimestamp == null || startingInsertTimestamp.isEmpty()) {
        timestampGenerator = randomizeTimestampOrder ? 
            new RandomDiscreteTimestampGenerator(timestampInterval, timeUnits, maxOffsets) :
            new UnixEpochTimestampGenerator(timestampInterval, timeUnits);
        // Set startTimestamp as first timestamp from the generator
        // (this also initialises the generator with the correct
        // initial timestamp, because the first call to nextValue()
        // is the one that sets & returns the correct initial timestamp
        startTimestamp = timestampGenerator.nextValue();
      } else {
        try {
          timestampGenerator = randomizeTimestampOrder ? 
              new RandomDiscreteTimestampGenerator(timestampInterval, timeUnits, 
                  Long.parseLong(startingInsertTimestamp), maxOffsets) :
              new UnixEpochTimestampGenerator(timestampInterval, timeUnits, 
                  Long.parseLong(startingInsertTimestamp));
          // timestampGenerator.currentValue() only returns the initial timestamp
          // *after* calling nextValue() the first time, so we call it
          // here.
          timestampGenerator.nextValue();
        } catch (NumberFormatException nfe) {
          throw new WorkloadException("Unable to parse the " + 
              CoreWorkload.INSERT_START_PROPERTY + " or (if transaction run and property is present) " +
              TimeSeriesWorkload.INSERT_TRANSACTION_START_PROPERTY, nfe);
        }
      }
      queryOffsetGenerator = new UniformLongGenerator(0, maxOffsets - 2);
    }
    
    /**
     * Generates the next write value for thread.
     * @param map An initialized map to populate with tag keys and values as well
     * as the timestamp and actual value.
     * @param isInsert Whether or not it's an insert or an update. Updates will pick
     * an older timestamp (if random isn't enabled).
     * @return The next key to write.
     */
    protected String nextDataPoint(final Map<String, ByteIterator> map, final boolean isInsert) {
      final Random random = ThreadLocalRandom.current();
      int iterations = sparsity <= 0 ? 1 : random.nextInt((int) ((double) perKeyCardinality * sparsity));
      if (iterations < 1) {
        iterations = 1;
      }
      while (true) {
        iterations--;
        if (timestampRollover) {
          timestampGenerator.nextValue();
          timestampRollover = false;
        }
        String key = null;
        // Only generate key values
        // and data if we are not doing a 'skip' iteration
        if (iterations <= 0) {
          final TreeMap<String, String> validationTags;
          if (dataintegrity) {
            validationTags = new TreeMap<String, String>();
          } else {
            validationTags = null;
          }
          int overallIdx;
          if (threadedWriteDistribution == "key") {
            key = keys[keyIdx];
            // cumulativeCardinality[0] holds the perKeyCardinality
            overallIdx = keyIdx * cumulativeCardinality[0];
          } else {
            key = keys[keyAndTagIndexes[0]];
            overallIdx = timeSeriesIndex; // they are the same;
          }

          for (int i = 0; i < tagPairs; ++i) {
            int tvidx;
            if (threadedWriteDistribution == "key") {
              tvidx = tagValueIdxs[i];
            } else {
              // the first element in keyAndTagIndexes is
              // the index for the key, the tag indexes start
              // from the second element
              tvidx = keyAndTagIndexes[i + 1];
            }
            map.put(tagKeys[i], new StringByteIterator(tagValues[tvidx]));
            if (dataintegrity) {
              validationTags.put(tagKeys[i], tagValues[tvidx]);
            }
            if (delayedSeries > 0 && threadedWriteDistribution == "key") {
              overallIdx += (tvidx * cumulativeCardinality[i + 1]);
            }
          }
          
          if (!isInsert) { // we are doing an UPDATE instead of INSERT
            final long delta = (timestampGenerator.currentValue() - startTimestamp) / timestampInterval;
            final int intervals = random.nextInt((int) delta);
            map.put(timestampKey, new NumericByteIterator(startTimestamp + (intervals * timestampInterval)));
          } else if (Math.abs(delayedSeries) > 2 * Double.MIN_VALUE) {
            // See if the series falls in a delay bucket and calculate an offset earlier
            // than the current timestamp value if so.
            double pct = (double) overallIdx / (double) totalCardinality;
            if (pct < delayedSeries) {
              int modulo = overallIdx % delayedIntervals;
              if (modulo < 0) {
                modulo *= -1;
              }
              map.put(timestampKey, new NumericByteIterator(timestampGenerator.currentValue() - 
                  timestampInterval * modulo));
            } else {
              map.put(timestampKey, new NumericByteIterator(timestampGenerator.currentValue()));
            }
          } else {
            map.put(timestampKey, new NumericByteIterator(timestampGenerator.currentValue()));
          }
          
          if (dataintegrity) {
            map.put(valueKey, new NumericByteIterator(validationFunction(key, 
                timestampGenerator.currentValue(), validationTags)));
          } else {
            switch (valueType) {
            case INTEGERS:
              map.put(valueKey, new NumericByteIterator(random.nextInt()));
              break;
            case FLOATS:
              map.put(valueKey, new NumericByteIterator(random.nextDouble() * (double) 100000));
              break;
            case MIXED:
              if (random.nextBoolean()) {
                map.put(valueKey, new NumericByteIterator(random.nextInt()));
              } else {
                map.put(valueKey, new NumericByteIterator(random.nextDouble() * (double) 100000));
              }
              break;
            default:
              throw new IllegalStateException("Somehow we didn't have a value "
                  + "type configured that we support: " + valueType);
            }        
          }
        }
        
        if (threadedWriteDistribution == "key") {
          boolean tagRollover = false;
          for (int i = tagCardinality.length - 1; i >= 0; --i) {
            if (tagCardinality[i] <= 1) {
              tagRollover = true; // Only one tag so needs roll over.
              continue;
            }
            
            if (tagValueIdxs[i] + 1 >= tagCardinality[i]) {
              tagValueIdxs[i] = 0;
              if (i == firstIncrementableCardinality) {
                tagRollover = true;
              }
            } else {
              // reset tagRollover to false (it may have been set to true earlier in the loop
              // because a tag has reached it last value or only has one value),
              // but this tag is still incrementing through values, so we are not done yet...
              tagRollover = false;
              ++tagValueIdxs[i];
              break;
            }
          }
          if (tagRollover) { // we are done iterating all the tag values
            if (keyIdx + 1 >= keyIdxEnd) { // we are done iterating all the key values, go to next timestamp
              keyIdx = keyIdxStart;
              timestampRollover = true;
            } else { // we are not done iterating key values, go to next key
              ++keyIdx;
            }
          }
        } else { // we are distributing by time series instead of by key
          ++timeSeriesIndex; // move onto the next time series
          if (timeSeriesIndex >= timeSeriesIndexEnd) {
            // we've reached the end of the index block for this thread,
            // time to return to the start and goto the next timestamp
            timeSeriesIndex = timeSeriesIndexStart;
            timestampRollover = true;
          }
          // Update the key and tag indexes
          for (int i = 0; i < keyAndTagCardinality.length; i++) {
            keyAndTagTreeIndexes[i] = timeSeriesIndex / (totalCardinality/ cumulativeTotalCardinality[i]);
            keyAndTagIndexes[i] = keyAndTagTreeIndexes[i] % keyAndTagCardinality[i];
          }
        }

        
        if (iterations <= 0) {
          return key;
        }
      }
    }
  }

}
