//******* BangDB engine related config *******
//IMPORTANT: most interesting and contextual (local to each app)  
//	params are marked with *** (3 stars) in the beginning of 
//      the comments. Little less interesting are marked with  ** (2 stars)
//      and still less with *(single star). Params that are not marked 
//	with any star, please be doubly sure before changing them

//***the dir where the db file will be created, Please edit it with 
//	suitable dir location default is the local dir, note this 
//	can be provided as input param while creating a database

SERVER_DIR = data

//The log file dir where BangDB will keep the db log file. Ideally the log
//file dir should be kept on hard disk since log file is always sequential and
//the data files (SERVER_DIR) should be kept on SSD. But there is no 
//restriction where actually you decide to keep the log files

BANGDB_LOG_DIR = logdir

//The archive dir where BangDB will keep archives of tables.

BANGDB_ARCHIVE_DIR = archive

//***the name of the db (applicable only for server and other clients 
//(including embedded db) should define the name while calling the API)
//for embedded db case, name is provided as input param while creating/opening
//a new database (hyphen '-' not allowed in the name)

BANGDB_DATABASE_NAME = mydb

//**type of persistence (0=INMEM_ONLY, 1=INMEM_PERSIST, 2=PERSIST_ONLY)
//applies to a table, hence within a db, different tables can have different persist
//types 0 (INMEM_ONLY) means data is only in memory not backed by any file on disk.
//If process terminates and data dump is not taken then data is lost. 

//option 1 (INMEM_PERSIST) means, data in memory is backed by file on disk 
//hence db can handle much more data than allocated memory, which is not possible 
//with option 0 (INMEM_ONLY)  

//Option 2 (PERSIST_ONLY) if other extreme where all operations are done with direct 
//I/O from file on the disk. Hence this is most conservative in nature with very 
//high data durability but with low performance. Not recommended!

BANGDB_PERSIST_TYPE = 1

//**type of index (1 = EXTHASH, 2 = BTREE)
//Applicable for a table hence within a db, different tables can have different 
//index types

BANGDB_INDEX_TYPE = 2

//***buf pool size (in MB)

BUFF_POOL_SIZE_HINT = 2048

//**whether to get support from SSD
//This allocates area in SSD drive and use to enhance the db performance, 
//esp in random I/O scenario. If don't wish to enable this (for ex;
//if SSD drive is not there) then put 0 as value else to enable
//specify the size of SSD dedicated for enahncing the performance
//value is in GB

BANGDB_SSD_POOL_SIZE = 64

//**if the BANGDB_SSD_POOL_SIZE is greater than 0 then we need to specify
//the dir on SSD drive where to create the pool

BANGDB_SSD_POOL_DIR = /home/ssd/pool

//***write ahead log enabled = 1, disabled = 0
//This is applicable for a table, hence within a db, different tables 
//can have different setting

BANGDB_LOG = 1

//**log buffer size (in MB)

LOG_BUF_SIZE = 128

//*syslog active. When active(1) then BangDB will do logging using syslog (/var/log/syslog file)
//when inactive(0) then BangDB will flush the logs on to standard output(terminal)
//and when set to 2 it will flush to the logfile maintain by the BangDB. The preferred
//value is 2 as BangDB implements high performance logging mechanism

BANGDB_APP_LOG = 0

//the db and app log size

DB_APP_LOG_SIZE_MB = 2

//**dat buffer or value size in KB (max)
//This is applicable for db hence applies to all tables

DAT_SIZE = 64

//**key size in bytes (max), min allowed size is 8 bytes
//Please keep this as low as possible in terms of size for better db performance and efficiency.
//This applies to a table hence within a db, different tables can have different key sizes. 
//But once set it can't be changed in future

KEY_SIZE = 24

//*the default max size of the resutset returned while range scan (in MB)
//the range scan query can limit the amount of data to be returned using this value

MAX_RESULTSET_SIZE = 2

//**to select the key comparison function. Note that once selected this will never 
//be allowed to change in future for a db. applicable only for Btree type. Default 
//value 1 is lexicographical order for keys and value 2 is for quasi lexicographical order
//for ex; for keys {12, 1, 2}, for value 1 order will be { 1, 12 ,2} and 
//with 2 it will be { 1, 2, 12} etc...

KEY_COMP_FUNCTION_ID = 2

//**Important when BangDB is run in transaction mode, If auto commit is off(0) then 
//explicit transaction is required (begin, commit/abort), else implicit non-transactional 
//single op can be run in usual manner later this can be set/unset whenever required 
//using the API exposed by connection

BANGDB_AUTOCOMMIT = 1

//*transaction cache size in terms of number of concurrent transactions. Increasing 
//this would decrease the probability of transaction getting aborted due to forced 
//reclaim of cache nodes. But default works well in most of the situation

BANGDB_TRANSACTION_CACHE_SIZE = 256

//**the default word size. This sets the size for typical word that we would deal with in any text.
//In normal engish language 35 is good enough, but since we deal with log files as well hence we
//need to allow user to set the typical max word size that they would like to use. This value
//should be as small as possible as it affects the size of index and performance
TEXT_WORD_SIZE = 48

//max number of tables a db can have

MAXTABLE = 128

//*max number of connections a client can have, note: all will share the bpool
//This should not be confused with maxnimum number of concurrent clients connections to 
//the server, for that see MAX_CLIENT_EVENTS

MAXCONN = 128

//*mainly for bangdb embedded not relevant for server
//max threads used by the client (application or client should not have more concurrent 
//threads than this number) need not be too accurate but it helps in optimizinf housekeeping

MAX_THREADS = 128

//*page size in bytes. This is constant for the db and applies to all tables.
// It can't be changed in future once set for large key_size, please set the
//page size larger. Basically we should try to have at least 32 keys in a page 
//but more key we can accomodate in a page, more efficient the computation 
//will be default size of 8192 works pretty well for keys upto 300 bytes length. 
//Try to design your keys such that it's as small as possible such as below 
//32 bytes

PAGE_SIZE_BANGDB = 8192

//**to decide the page split percentage for BTREE. Default is 50%. But in case of sequential writes
//it is helpful and performant to have the splitting page retain most of the items. To increase
//the percentage of retained items in the splitting page, increase the value here, for example for 
//sequential write scenario 90 would be a good value whereas for random operations 50 is better
//(min is 50 and max is 100)

PAGE_SPILT_FACTOR = 60

//*master log size (in KB), keep it low, typically 64KB is very high for several
//billions of records

MASTER_LOG_BUF_SIZE = 64

//**the sync for db data while close, 0 means no sync else sync

BANGDB_SYNC_TRAN = 1

//**register signal handler with 1 and de-register with value 0

BANGDB_SIGNAL_HANDLER_STATE = 0

//**the frequency for log flush in micro sec

LOG_FLUSH_FREQ = 1000000

//***to enable(1)/disable(0) check pointing

CHKPNT_ENABLED = 0

//*check point frequency in micro sec

CHKPNT_FREQ = 3370000

//*log split check frequency in micro sec

LOG_SPLIT_CHECK_FREQ = 23000

//*the buffer cache dirty page flusher and the buffer cache memory reclaimer frequency 
//in micro sec note that this is just a hint and db changes this as per need

BUF_FLUSH_RECLAIM_FREQ = 60000

//in case of deep pressure on memory, grow the buffer by amount in MB (Not used as of now)

GROW_BUFF_SIZE = 16

//*max pages to look for scatter gather, put 0 to select the system supported 
//number (suggested), else put whatever num, but if it's more than system supported
//then it will be corrected to the system suppored one

SCATTER_GATHER_MAX = 0

//please ensure before changing the parameters below as they may affect the db's performance

//max hdrs to scan to look for dirty pages

MIN_DIRTY_SCAN = 128

//max headers to scan to find a updated page

MIN_UPDATED_SCAN = 32

//this defines the constraints for flushing the index pages

IDX_FLUSH_CONSTRAINT = 4

//this defines the constraints for flushing the data pages

DAT_FLUSH_CONSTRAINT = 25

//this defines the constraints for freeing up the index pages for memory

IDX_RECLAIM_CONSTRAINT = 3

//this defines the constraints for freeing up the dat pages for memory

DAT_RECLAIM_CONSTRAINT = 7

//this indicates the speed at which the data is written

PAGE_WRITE_FACTOR = 128

//this indicates the speed at which data is read

PAGE_READ_FACTOR = 128

//this normalizes the idx vs dat pages, helpful when we favor one over other

IDX_DAT_NORMALIZE = 2

//the pre-fetch buffer max size, it can be lower than this but not greater (in MB)

PREFETCH_BUF_SIZE = 2

//pre-fetch scan window size

PREFETCH_SCAN_WINDOW_NUM = 20

//pre-fetch extent size

PREFETCH_EXTENT_NUM = 16

//*************************************************************************************
//******* server configuration (applicable for master - slave model network db) *******
//*************************************************************************************

//***type of server master(0) or slave(1) (only for servers and not for the client)

SERVER_TYPE = 0

//**enable or disable replication

ENABLE_REPLICATION = 1

//***the ip address or name of the server

SERVER_ID = 127.0.0.1

//***port number where db service is running

SERV_PORT = 10101

//***in case of slave set the following two appropriately, note for master 
//node these two will be same as SERVER_ID and SERV_PORT
//the ip address or name of the master server

MASTER_SERVER_ID = 127.0.0.1

//***master's port num

MASTER_SERV_PORT = 10101

//**2nd argument to listen(), the queue size for listen

LISTENQ = 10000

//***max slaves allowed in the master slave topology

MAX_SLAVES = 4

//**the ops record buffer size in MB(useful in replication). 
//for 10M ops 500 MB should be good enough

OPS_REC_BUF_SIZE = 256

//***the ping frequency (default = 10s)

PING_FREQ = 10

//***the threshold for ping failure (num of times, default is 5)

PING_THRESHOLD = 5

//***the timeout value for client sockets, 0 means no timeout and 
//positive value means time out in seconds

CLIENT_TIME_OUT = 120

//**maximum number of concurrent connections to the server or num of concurrent connections 
//server can handle default is 10000, but change it to less number as suitable. Please don't 
//go beyond 10000 as of now not relevant for bangdb embedded

MAX_CLIENT_EVENTS = 10000

//*stage options, basically it tells server to create the number of stages 
//to handle the clients and their requests there are two types of stages supported as of now
//1. two stages, one for handling clients and other for handling the requests
//2. four stages, one for handling clients, one for read, one for ops and finally one for write
//default is option 1

SERVER_STAGE_OPTION = 3

//*how many workers for the ops stage. Note that for option 1, the workers would 
//be for all the last three events (read, write, ops) suggested value for num of workers for 
//option 1 is the number of processors on the machine and that's the default but you may change it
//for option 2, you should make it NPROC-2 (suggested for NPROC>2, else 1, 
//this is default too for option 2) but you can choose any number
//value 0 means default else the exact num of workers

SERVER_OPS_WORKERS = 0

//***** Don't bother for these ******
//bangdb group id

BANGDB_GROUP = bangdb

//***the name of the table (applicable only for server and other clients 
//(including embedded db) should define the name while calling the API)
//for embedded db case, name is provided as input param while creating/opening 
//a table (hyphen '-' not allowed in the name)

BANGDB_TABLE_NAME = bangdbTableDdata

// TLS identity and psk key: 32 char hexadecimal
TLS_IDENTITY = 10
TLS_PSK_KEY = 1A1A1A1A1A1A1A1A1A1A1A1A1A1A1A1A

//location to keep static data tables
STATIC_TABLE_LOC =  static_data_dir

//Max num of connections in the pool
//keep it short, best number would be order of numOfCores on the machines
NUM_CONNECTIONS_IN_POOL = 16

