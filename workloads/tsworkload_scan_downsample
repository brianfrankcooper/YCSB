# Copyright (c) 2017 YCSB contributors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you
# may not use this file except in compliance with the License. You
# may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License. See accompanying
# LICENSE file.

# Yahoo! Cloud System Benchmark
# Workload A: Small cardinality consistent data for 2 days
#   Application example: Typical monitoring of a single compute or small 
#   sensor station where 90% of the load is write and only 10% is read 
#   (it's usually much less). All writes are inserts. No sparsity so 
#   every series will have a value at every timestamp.
#
#   Read/insert ratio: 10/90
#   Cardinality: 16 per key (field), 64 fields for a total of 1,024 
#                time series.
workload=site.ycsb.workloads.TimeSeriesWorkload

recordcount=200
operationcount=200
insertcount=200
insertstart=1578836323
inserttransactionstart=1578839923

fieldlength=8
fieldcount=5
tagcount=3
tagcardinality=2,1,2

sparsity=0.0
delayedseries=0.0
delayedintervals=0

timestampunits=SECONDS
timestampinterval=60
querytimespan=600
randomtimeseriesorder=false

# A function name (e.g. 'sum', 'max' or 'avg') passed during reads and
# scans to cause the database to perform a downsampling operation
# returning lower resolution data. If this value is empty or null 
# (default), downsampling is not performed.
downsamplingfunction=SUM

# A time interval for which to downsample the raw data into. Shares
# the same units as 'timestampinterval'. This value must be greater
# than 'timestampinterval'. E.g. if the timestamp interval for raw
# data is 60 seconds, the downsampling interval could be 3600 seconds
# to roll up the data into 1 hour buckets.
downsamplinginterval=300

scanproportion=1.00
readproportion=0.00
insertproportion=0.00
updateproportion=0.00
